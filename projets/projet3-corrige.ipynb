{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enoncé et consignes\n",
    "===================\n",
    "\n",
    "Le propos de ce projet est de finaliser l'implantation d'un parser en dépendances par transitions à partir d'une implémentation limitée au tagging qui est proposée ci-dessous. \n",
    "\n",
    "Ce parser s'appuie sur un modèle d'analyse neuronal où la phrase d'entrée est encodée par un bi-lstm, ce qui servira de features pour prendre les décisions de transition.\n",
    "\n",
    "Le parser utilisera l'algorithme arc-standard et utilisera une méthode de recherche de solutions gloutonne.\n",
    "\n",
    "Le jeu de données utilisé est une verions du corpus sequoia : \n",
    "    https://gforge.inria.fr/frs/?view=shownotes&group_id=3597&release_id=9064.\n",
    "qui a la propriété d'être projectif. \n",
    "Le fichier de données utilisé est `sequoia-corpus.np_conll`\n",
    "\n",
    "\n",
    "\n",
    "Modèle d'apprentissage\n",
    "---------------------\n",
    "Le parser à réaliser sera un hybride entre celui proposé par Kipperwasser et Golbderg (2016)\n",
    "et le système multitâche pour l'analyse en constituants décrit par Coavoux et Crabbé (2017).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consignes spécifiques:\n",
    "---------------------\n",
    "\n",
    "1. Lire attentivement le code proposé ci-dessous et en discuter entre vous. Vous pouvez exécuter et tester le code existant il s'agit d'un tagger parfaitement fonctionnel.\n",
    "2. Changer le système de transition :\n",
    "    * Modifier la méthode `index_symbols` pour qu'elle encode sur le modèle des actions shift les actions de type left arc right arc sous forme de couple à deux éléments associés à des entiers\n",
    "    * Implémenter les méthodes `left_arc`, `right_arc` \n",
    "    * Modifier la méthode `oracle` pour qu'elle implémente l'oracle statique arc-standard \n",
    "    * Modifier la méthode `forward` pour que le critère d'arrêt de la boucle corresponde au critère d'arrêt du système de transition que vous utilisez et que les transitions left-arc et right-arc soient effectivement utilisées. \n",
    "3. Implémenter une méthode `predict_tree(...)` inspirée de `predict_tags` qui renvoie l'arbre de dépendances prédit\n",
    "sous forme d'un objet `DependencyTree`.\n",
    "4. Implémenter une fonction `eval_parser` inspirée de `eval_tagger` qui renvoie le LAS et l'UAS de votre parser.    \n",
    "5. Features pour le parser. A ce stade, le parser devrait renvoyer des résultats la plupart du temps incorrects car la décision d'analyse est prise uniquement sur base de la représentation du premier élément du buffer.\n",
    "   * Modifier le modèle pour que cette décision s'appuie également sur les représentations des deux (ou trois) éléments situés sur la pile.\n",
    "   * Evaluer l'impact de vos modifications et comparer par rapport à ce qui précède. Reporter vos UAS/LAS à chaque fois  \n",
    "6. Gestion des mots inconnus. Suite à vos premières évaluations, vous aurez peut-être remarqué que le tagger ou le parser a des performances relativement moyennes. Pour améliorer la situation, on propose d'incorporer un module de gestion des mots inconnus.\n",
    "    * Implanter une méthode `word_dropout(...,alpha=1.0)` qui prend en paramètre une liste de mots et qui renvoie cette liste avec certains mots remplacés par le token du mot inconnu. En général on choisit de remplacer un mot de basse fréquence $w$ par le token inconnu avec la probabilité $p = \\frac{\\alpha}{count(w)} $ où $\\alpha \\in [0,1]$ mais vous êtes libre de choisir votre méthode.  \n",
    "    * Implanter une classe de modèle de caractère équipée d'une méthode `forward(...)` qui prend en entrée une string et qui renvoie un vecteur calculé par un modèle RNN de caractères. Intégrer ce module au modèle d'analyse syntaxique pour améliorer la gestion des mots inconnus (**Exercice plus compliqué**, mais vous pouvez vous inspirer de tutoriels existants sur le web)\n",
    "    * Evaluer l'impact de vos modifications et comparer par rapport à la baseline. Reporter vos UAS/LAS à chaque fois    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import random\n",
    "\n",
    "from tqdm import tqdm #optional, for progress bars in notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure de données annexe\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyTree:\n",
    "\n",
    "    DUMMY_ROOT = '$ROOT$'\n",
    "    \n",
    "    def __init__(self,wordlist,edges=None):\n",
    "        \n",
    "            self.wordlist = wordlist                  #hyp: DUMMY ROOT is wordlist[0]  \n",
    "            self.edges    = edges if edges else []    #edges are 4-tuples (govidx,label,deptag,depidx)\n",
    "            self.edges.sort(key = lambda x:x[3])\n",
    "                \n",
    "    def tag_list(self):\n",
    "        return [deptag for (gidx,deplabel,deptag,didx) in self.edges]\n",
    "    def word_list(self):\n",
    "        return self.wordlist\n",
    "    def dlabel_list(self):\n",
    "         return [deplabel for (gidx,deplabel,deptag,didx) in self.edges]\n",
    "    \n",
    "    #aux functions useful for oracle\n",
    "    def get_edge(self,gov_idx,dep_idx):\n",
    "       #gets the edge (with labels) from gov_idx to dep_idx \n",
    "       #returns either a singleton or an empty list\n",
    "       return [ (gidx,x,y,didx) for (gidx,x,y,didx) in self.edges \\\n",
    "                                if gidx == gov_idx and didx == dep_idx]\n",
    "\n",
    "    def dom_edges(self,gov_idx):\n",
    "         #gets all edges couples from gov_idx to some dependant node\n",
    "         return [(gidx,didx) for (gidx,_,_,didx) in self.edges if gidx == gov_idx]\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_tree(istream):\n",
    "        \"\"\"\n",
    "        Reads a Conll-u tree and returns it as a DepTree object\n",
    "        returns None if no tree has been read\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        edges = []\n",
    "        for line in istream:\n",
    "            line = line.split(\"#\")[0]\n",
    "            if line and not line.isspace():\n",
    "                values = line.split()\n",
    "                words.append(values[1])\n",
    "                edges.append( (int(values[6]),values[7], values[3], int(values[0])) )\n",
    "            elif words:\n",
    "                return DependencyTree([DependencyTree.DUMMY_ROOT] + words,edges)\n",
    "        return DependencyTree([DependencyTree.DUMMY_ROOT] + words,edges) if words else None\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Pretty prints a tree for debug\n",
    "        Returns: a string\n",
    "        \"\"\"\n",
    "        lines =  [(didx,self.wordlist[didx],dtag,dlabel,gidx) for (gidx,dlabel,dtag,didx) in self.edges] \n",
    "        lines.sort(key=lambda x:x[0])\n",
    "        return '\\n'.join( [ \"%d\\t%s\\t%s\\t%s\\t%d\"%(L) for L in lines ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training trees 14754\n",
      "#validation trees 1235\n",
      "#test trees 2540\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_projective(root_idx,edges):\n",
    "    rspan = [root_idx]\n",
    "    children = [ didx for (gidx,_,_,didx) in edges if gidx==root_idx ]\n",
    "    for child in children:\n",
    "        proj,span = is_projective(child,edges)\n",
    "        if not proj:\n",
    "            return False,[]\n",
    "        rspan.extend(span)\n",
    "    rspan.sort()\n",
    "    return  ( all([ jdx == idx+1 for idx,jdx in zip(rspan,rspan[1:])]),rspan )\n",
    "    \n",
    "def read_trees(filename):\n",
    "\n",
    "    istream = open(filename)\n",
    "    \n",
    "    ilist = []\n",
    "    dtree = DependencyTree.read_tree(istream)\n",
    "    while dtree is not None:\n",
    "        proj,span = is_projective(0,dtree.edges)\n",
    "        if proj:\n",
    "            ilist.append(dtree)\n",
    "        #else:\n",
    "        #    print('[warning] non projective tree skipped')\n",
    "        dtree = DependencyTree.read_tree(istream)\n",
    "        \n",
    "    istream.close()\n",
    "    return ilist\n",
    "        \n",
    "train_treebank = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/train.French.gold.conll\")\n",
    "valid_treebank = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/dev.French.gold.conll\")\n",
    "test_treebank  = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/test.French.gold.conll\")\n",
    "\n",
    "print('#training trees',len(train_treebank))\n",
    "print('#validation trees',len(valid_treebank))\n",
    "print('#test trees',len(test_treebank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codage\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sym2idx(wordlist,unk_symbol=None):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping symbols to integers\n",
    "    Args:\n",
    "       wordlist  (list): a list of strings\n",
    "       unk_symbol (str): a special default string for symbols unknown to this dictionary\n",
    "    Returns:\n",
    "        a dict string => idx mapping words to integer indexes\n",
    "    \"\"\"\n",
    "    if unk_symbol:\n",
    "        wordlist.append(unk_symbol)\n",
    "    wordset = set(wordlist)\n",
    "    return dict(zip(wordset,range(len(wordset)))) \n",
    "\n",
    "def code_sequence(symlist,sym2idx,unk_symbol=None):\n",
    "    \"\"\"\n",
    "    Maps a list of string to a list of int (encodes the sentence on integers) \n",
    "    Args:\n",
    "        symlist  (list): a list of strings\n",
    "        sym2idx  (dict): a dictionary mapping strings to int\n",
    "        unk_symbol(str): a special default string for the dictionary sym2idx\n",
    "    Returns a list of int as torch.tensor where words are mapped to their integer indexes.\n",
    "    \"\"\"\n",
    "    def normal_form(symbol):\n",
    "        return symbol if symbol in sym2idx else unk_symbol\n",
    "    \n",
    "    code_list = [sym2idx[normal_form(symbol)] for symbol in symlist]\n",
    "    return torch.tensor(code_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(nn.Module):\n",
    "      \n",
    "    #ACTIONS\n",
    "    SHIFT = \"S\"\n",
    "    LARC  = \"L\"\n",
    "    RARC  = \"R\"\n",
    "    \n",
    "    #DUMMY TAG\n",
    "    DTAG  = 'ROOT'\n",
    "    \n",
    "    def __init__(self,embedding_dim,lstm_memory_dim,config_embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim        (int): size of word embeddings\n",
    "            lstm_memory_dim      (int): size of the sentence encoder lstm memory \n",
    "            config_embedding_dim (int): size of the hidden layer of the output MLP.\n",
    "        \"\"\"\n",
    "        super(ArcStandardParser, self).__init__()\n",
    "        self.embedding_dim        = embedding_dim\n",
    "        self.lstm_memory_dim      = lstm_memory_dim\n",
    "        self.config_embedding_dim = config_embedding_dim\n",
    "\n",
    "        self.char_embedding_dim   = 32\n",
    "        self.char_lstm_memory_dim = 64 \n",
    "        \n",
    "    ### INDEXING ###\n",
    "    def index_symbols(self,treebank,unk_word,unk_char):\n",
    "        \"\"\"\n",
    "        Indexes the x and y symbols on integers\n",
    "        Args:\n",
    "           treebank   (list): a list of DependencyTree objects\n",
    "           unk_word (string): a symbol to use for unk words\n",
    "           unk_char (string): a symbol to use for unk char\n",
    "        Returns:\n",
    "           (a dict word -> int, a dict action -> int)\n",
    "        \"\"\"\n",
    "        wlist      = []   \n",
    "        taglist    = [ArcStandardParser.DTAG]\n",
    "        actionlist = [(ArcStandardParser.SHIFT,None)]\n",
    "        charset    = set([])\n",
    "        for dtree in treebank:\n",
    "            for w in dtree.word_list():\n",
    "                charset.update(list(w))\n",
    "            wlist.extend(dtree.word_list())\n",
    "            taglist.extend(dtree.tag_list())\n",
    "            actionlist.extend([ (ArcStandardParser.LARC,dlab) for dlab in dtree.dlabel_list() ] )\n",
    "            actionlist.extend([ (ArcStandardParser.RARC,dlab) for dlab in dtree.dlabel_list() ] )\n",
    "        w2idx         = make_sym2idx(wlist,unk_symbol=unk_word)\n",
    "        tag2idx       = make_sym2idx(taglist)\n",
    "        wcounts       = Counter(wlist)\n",
    "        action2idx    = make_sym2idx(actionlist)\n",
    "        char2idx      = make_sym2idx(wlist,unk_symbol=unk_char)\n",
    "        return w2idx,tag2idx,action2idx,wcounts,char2idx\n",
    "    \n",
    "    ### MODEL STRUCTURE AND INFERENCES ###\n",
    "    def allocate_structure(self,nWords,nTags,nActions,nChars):\n",
    "        \"\"\"\n",
    "        Allocates memory for the parser network params.\n",
    "        Args:\n",
    "           nWords  (int):number of words in the sym2idx dict\n",
    "           nTags   (int):number of tags  in the parser\n",
    "           nActions(int):number of actions in the parser\n",
    "           nChars  (int):number of chars inthe char2idx dict\n",
    "        \"\"\"\n",
    "        self.word_embeds           = nn.Embedding(nWords, self.embedding_dim)\n",
    "        self.tagger_bilstm         = nn.LSTM(self.embedding_dim+self.char_lstm_memory_dim, self.lstm_memory_dim,num_layers=1,bidirectional=True)\n",
    "        self.parser_bilstm         = nn.LSTM(self.lstm_memory_dim*2, self.lstm_memory_dim,num_layers=1,bidirectional=True)\n",
    "        self.tagger_lstm2hidden    = nn.Linear(self.lstm_memory_dim*2,self.config_embedding_dim)\n",
    "        self.hidden2tags           = nn.Linear(self.config_embedding_dim,nTags)\n",
    "        self.parser_lstm2hidden    = nn.Linear(self.lstm_memory_dim*8,self.config_embedding_dim)\n",
    "        self.hidden2actions        = nn.Linear(self.config_embedding_dim,nActions)\n",
    "       \n",
    "        self.char_embeds    = nn.Embedding(nChars, self.char_embedding_dim)\n",
    "        self.char_bilstm    = nn.LSTM(self.char_embedding_dim, self.char_lstm_memory_dim,num_layers=1,bidirectional=True)        \n",
    "        self.char_lstm2word = nn.Linear(self.char_lstm_memory_dim*2,self.char_lstm_memory_dim)\n",
    "    \n",
    "    \n",
    "    def word_dropout(self,toklist,alpha=1.0):\n",
    "        \"\"\"\n",
    "        Replaces each word in toklist with probability Lambda / counts(w)\n",
    "        Args:\n",
    "            toklist (list): a list of strings\n",
    "            alpha  (float): a real in [0,1]. a word w is replaced with prob = alpha/counts(w)\n",
    "        Returns:\n",
    "            a list of strings with some strings replace by the unk token\n",
    "        \"\"\"\n",
    "        def sample_word(wordform,unk_form):\n",
    "            r = random()\n",
    "            if r < alpha/self.wcounts[wordform]:\n",
    "                return unk_form\n",
    "            return wordform\n",
    "    \n",
    "        if alpha == 0:     #no dropout\n",
    "            return toklist\n",
    "        \n",
    "        return [sample_word(token,self.unk_word) for token in toklist]\n",
    "         \n",
    "    def train_model(self,train_set,validation_set,epochs=10,learning_rate=0.1,unk_word='##UNK##',unk_char='@@',alpha=0.5):\n",
    "                    \n",
    "        self.w2idx, self.tag2idx,self.a2idx,self.wcounts,self.char2idx = self.index_symbols(train_set,unk_word,unk_char)\n",
    "        \n",
    "        self.reva2idx   = [action for action,idx in sorted(self.a2idx.items(),key = lambda x : x[1])]\n",
    "        self.revtag2idx = [tag for tag,idx in sorted(self.tag2idx.items(),key = lambda x : x[1])]\n",
    "        \n",
    "        self.unk_word,self.unk_char = unk_word,unk_char\n",
    "        self.allocate_structure(len(self.w2idx),len(self.tag2idx),len(self.a2idx),len(self.char2idx))\n",
    "        \n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer     = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        min_loss      = np.iinfo(np.int32).max\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            NLL = 0.0\n",
    "            for deptree in tqdm(train_set): \n",
    "                \n",
    "                self.zero_grad()                     \n",
    "                \n",
    "                X               = deptree.word_list()\n",
    "                xtagger,xparser = self.forward_encoding(X,alpha)\n",
    "\n",
    "                ypred,yref      = self.forward_tagger(xtagger,oracle_tree=deptree) # ypred is a sequence of y-softmaxed-probs for each step in the inference                      \n",
    "                loss            = loss_function(ypred, yref)\n",
    "                NLL            += loss.item()\n",
    "                loss.backward(retain_graph=True)\n",
    "                    \n",
    "                ypred,yref      = self.forward_parser(xparser,oracle_tree=deptree)\n",
    "                loss            = loss_function(ypred, yref)\n",
    "                NLL            += loss.item()\n",
    "                loss.backward()\n",
    "              \n",
    "                optimizer.step()\n",
    "                \n",
    "            print(\"\\n[train]      Epoch %d, NLL = %f\"%(epoch,NLL/len(train_set)),file=sys.stderr)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                NLL = 0.0\n",
    "                for deptree in validation_set:\n",
    "                     X               = deptree.word_list()\n",
    "                     xtagger,xparser = self.forward_encoding(X)\n",
    "                        \n",
    "                     ypred,yref      = self.forward_tagger(xtagger,oracle_tree=deptree)      # ypred is a sequence of y-softmaxed-probs for each step in the inference\n",
    "                     loss            = loss_function(ypred, yref) \n",
    "                     NLL            += loss.item()\n",
    "                    \n",
    "                     ypred,yref      = self.forward_parser(xparser,oracle_tree=deptree)\n",
    "                     loss            = loss_function(ypred, yref)\n",
    "                     NLL            += loss.item()\n",
    "                if NLL < min_loss:\n",
    "                    torch.save(self.state_dict(), 'parsing_model.wt')        \n",
    "                print(\"[validation] Epoch %d, NLL = %f\\n\"%(epoch,NLL/len(validation_set)),file=sys.stderr)  \n",
    "            \n",
    "        self.load_state_dict(torch.load('parsing_model.wt'))\n",
    "\n",
    "        \n",
    "    def forward_encoding(self,wordlist,alpha=0.0):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for encoding the input only.\n",
    "        Args:\n",
    "           wordlist   (list): a list of strings\n",
    "           alpha     (float): 0 <= alpha <= 1 float for word dropout\n",
    "        Returns:\n",
    "           two lists of vectors encoding the sentence to be used for tagging and parsing the input\n",
    "           as torch.tensor\n",
    "        \"\"\"        \n",
    "        xwords                = code_sequence(self.word_dropout(wordlist,alpha),self.w2idx,unk_symbol=self.unk_word) \n",
    "        xword_embedded        = self.word_embeds(xwords)\n",
    "        xchar_embedded        = []\n",
    "        for w in wordlist:\n",
    "            char_codes        = code_sequence(list(w),self.char2idx,unk_symbol=self.unk_char) \n",
    "            char_embedded     = self.char_embeds(char_codes)\n",
    "            _,(char_hidden,_) = self.char_bilstm(char_embedded.view(len(w), 1, -1), None)\n",
    "            xchar_embedded.append(self.char_lstm2word(char_hidden.view(2*self.char_lstm_memory_dim)))\n",
    "        \n",
    "        xchar_embedded        = torch.stack(xchar_embedded)\n",
    "        xcodes                = torch.cat([xword_embedded,xchar_embedded],1)\n",
    "\n",
    "        xtagger, hidden_tag   = self.tagger_bilstm(xcodes.view(len(xcodes), 1, -1), None)\n",
    "        xparser, hidden_par   = self.parser_bilstm(xtagger, None)\n",
    "        return (xtagger,xparser)\n",
    "        \n",
    "        \n",
    "    def forward_tagger(self,xtagger,oracle_tree=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for tagging only.\n",
    "        Args:\n",
    "           xtagger               (list): a list of vectors,one per word,encoding the input sentence\n",
    "           oracle_tree (DependencyTree): an optional Dependency tree used as oracle\n",
    "        Returns:\n",
    "           a list of tags (one per word, including the dummy word)\n",
    "           or\n",
    "           a list of softmax distributions as torch.tensor and the list of reference tags if an oracle_tree is provided\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden        = F.relu(self.tagger_lstm2hidden(xtagger.squeeze()))\n",
    "        softmaxin     = self.hidden2tags(hidden)\n",
    "        log_softmaxes = F.log_softmax(softmaxin, dim=1)  \n",
    "\n",
    "        if oracle_tree:\n",
    "            ref_tags = [ArcStandardParser.DTAG] + oracle_tree.tag_list()\n",
    "            ref_tags = code_sequence(ref_tags,self.tag2idx)\n",
    "            return (log_softmaxes,ref_tags)\n",
    "        else:\n",
    "            max_probs,  argmaxlist  = torch.max(log_softmaxes,1)\n",
    "            pred_tags               = [self.revtag2idx[argmax] for argmax in argmaxlist]\n",
    "            return pred_tags\n",
    "        \n",
    "    def forward_parser(self,xparser,oracle_tree=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for parsing only.\n",
    "        Args:\n",
    "           xparser               (list): a list of vectors,one per word,encoding the input sentence\n",
    "           oracle_tree (DependencyTree): an optional Dependency tree used to drive the oracle\n",
    "        Returns:\n",
    "           the final configuration\n",
    "           or\n",
    "           a list of softmax distributions, and the list of reference actions if an oracle_tree is provided\n",
    "        \"\"\"        \n",
    "        \n",
    "        yprobs     = [] #sequence of predictions\n",
    "        yref       = [] #sequence of gold actions\n",
    "        \n",
    "        config     = self.init_configuration(len(xparser))\n",
    "        S,B,A      = config\n",
    "        \n",
    "        while B or len(S) >= 2:\n",
    "            \n",
    "            #predict action probs with MLP\n",
    "            S1 = xparser[ S[-1] ] if S          else torch.zeros(1,self.lstm_memory_dim*2)\n",
    "            S2 = xparser[ S[-2] ] if len(S) > 1 else torch.zeros(1,self.lstm_memory_dim*2)\n",
    "            S3 = xparser[ S[-3] ] if len(S) > 2 else torch.zeros(1,self.lstm_memory_dim*2)\n",
    "            B0 = xparser[ B[0]  ] if B          else torch.zeros(1,self.lstm_memory_dim*2)\n",
    "        \n",
    "            xfeatures     = torch.cat([S3,S2,S1,B0],1)\n",
    "            hidden        = F.relu(self.parser_lstm2hidden(xfeatures))\n",
    "            \n",
    "            softmaxin     = self.hidden2actions(hidden) + self.run_time_mask(config)\n",
    "            log_softmaxes = F.log_softmax(softmaxin, dim=1)  \n",
    "            \n",
    "            #best action\n",
    "            if oracle_tree:\n",
    "                selected_action      = self.oracle(config,oracle_tree)\n",
    "                yref.append(selected_action)\n",
    "                yprobs.append(log_softmaxes)\n",
    "            else:\n",
    "                max_prob,  argmax    = torch.max(log_softmaxes,1)\n",
    "                selected_action      = self.reva2idx[argmax]\n",
    "\n",
    "            #update config\n",
    "            act_type,act_label   = selected_action\n",
    "            if act_type   == ArcStandardParser.SHIFT:\n",
    "                config           = self.shift(config)\n",
    "            elif act_type == ArcStandardParser.LARC:\n",
    "                config           = self.left_arc(config,act_label)\n",
    "            elif act_type == ArcStandardParser.RARC:\n",
    "                config           = self.right_arc(config,act_label)\n",
    "            S,B,A = config\n",
    "                    \n",
    "        if oracle_tree:\n",
    "            yprobs = torch.cat(yprobs)\n",
    "            yref   = code_sequence(yref,self.a2idx)\n",
    "            return (yprobs,yref)\n",
    "        else:\n",
    "            return config\n",
    "        \n",
    "    def predict_tree(self,wordlist):\n",
    "        \"\"\"\n",
    "        Actually performs the tagging and parsing of a sentence.\n",
    "        Args:\n",
    "            wordlist (list): a list of strings\n",
    "        Returns:\n",
    "            a tree  (DependencyTree): the dep tree and its tags\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            xtagger,xparser  = self.forward_encoding(wordlist)\n",
    "            taglist          = self.forward_tagger(xtagger)\n",
    "            S,B,A            = self.forward_parser(xparser) \n",
    "            edges            = [(gidx,deplabel,taglist[didx],didx)  for (gidx,deplabel,didx) in A]\n",
    "            return           DependencyTree(wordlist,edges)    \n",
    "        \n",
    "    ### TRANSITION SYSTEM ###\n",
    "    def init_configuration(self,N):\n",
    "        \"\"\"\n",
    "        Creates an init configuration for an arc standard parser.\n",
    "        A configuration is a triple (stack_idxes,buffer_idxes,dep_arcs)\n",
    "        Args:\n",
    "            N  (int): the length of the input\n",
    "        Returns:\n",
    "            the init configuration\n",
    "        \"\"\"\n",
    "        return ([0],list(range(1,N)),[]) \n",
    "        # configs stack and buffer are filled with integer indexes of word positions\n",
    "        # stack starts with [0] the dummy root node\n",
    "        \n",
    "    def shift(self,configuration):\n",
    "        \"\"\"\n",
    "        Performs shift on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration to be shifted\n",
    "        Returns: \n",
    "           the shifted configuration\n",
    "        \"\"\"\n",
    "        stack,buffer,arcs = configuration\n",
    "        return (stack + [buffer[0]],buffer[1:],arcs)\n",
    "                \n",
    "    def left_arc(self,configuration,deplabel):\n",
    "        \"\"\"\n",
    "        Performs a left arc action on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "           deplabel     (string): a string labelling the arc\n",
    "        Returns: \n",
    "           the resulting configuration\n",
    "        \"\"\"\n",
    "        stack,buffer,arcs = configuration\n",
    "        i = stack[-2]\n",
    "        j = stack[-1]\n",
    "        return (stack[:-2]+[j],buffer,arcs+[(j,deplabel,i)])\n",
    "        \n",
    "    def right_arc(self,configuration,deplabel):\n",
    "        \"\"\"\n",
    "        Performs a right arc action on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "           deplabel     (string): a string labelling the arc\n",
    "        Returns: \n",
    "           the resulting configuration\n",
    "        \"\"\"\n",
    "        stack,buffer,arcs = configuration\n",
    "        i = stack[-2]\n",
    "        j = stack[-1]\n",
    "        return (stack[:-1],buffer,arcs+[(i,deplabel,j)])\n",
    "\n",
    "    def run_time_mask(self,configuration,log_mask=True):\n",
    "        \"\"\"\n",
    "        The run time mask returns a mask where impossible actions have 0 score.\n",
    "        The mask is a vector of mask pseudo scores with dimensions comparable to the output action score vector.\n",
    "        Impossible actions are filled with a -inf value, and possible actions with a 0 value.\n",
    "        \n",
    "        The mask can be added to action scores prior to softmax.\n",
    "        Using the mask ensures that we have no fatal error while performing oracle-less runtime search.\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "        Kwargs:\n",
    "            log_mask      (bool): bool stating if the mask is in logit_space or in probabilistic space.\n",
    "        Returns:\n",
    "            torch.tensor of size num Actions filled with masking values\n",
    "        \"\"\"\n",
    "        m0,m1 = (-float('inf'), 0.0) if log_mask else (0.0,1.0)\n",
    "        \n",
    "        def valmask(atype,stack_size,buffer_size,configuration):\n",
    "            if atype in [ArcStandardParser.LARC,ArcStandardParser.RARC] and stack_size < 2:\n",
    "                return m0\n",
    "            elif atype == ArcStandardParser.LARC and stack_size >= 2:\n",
    "                S,B,A = configuration\n",
    "                if S[-2] == 0:\n",
    "                    return m0\n",
    "            elif atype == ArcStandardParser.SHIFT and buffer_size == 0:\n",
    "                return m0\n",
    "            return m1\n",
    "            \n",
    "        S,B,A       = configuration\n",
    "        stack_size  = len(S)\n",
    "        buffer_size = len(B)\n",
    "        return torch.tensor([ valmask(atype,stack_size,buffer_size,configuration)   for (atype,alabel) in self.reva2idx])  \n",
    "\n",
    "    def oracle(self,config,deptree):\n",
    "        \"\"\"\n",
    "        Deterministic function translating a dependency tree into a derivation sequence step by step.\n",
    "        Arc standard static oracle.\n",
    "        Args:\n",
    "            config           (tuple): a configuration\n",
    "            deptree (DependencyTree): the tree used as ground truth\n",
    "        Returns:\n",
    "            the action as a tuple (ActionType,ActionLabel)\n",
    "        \"\"\"\n",
    "        def node_is_complete(nidx,ref_tree,dep_arcs):\n",
    "            #says if all ref arcs leaving from this node in the reference are already in dep arcs (true) or not (false)\n",
    "            ref_edges  = set(ref_tree.dom_edges(nidx))\n",
    "            pred_edges = set( [(gidx,didx) for (gidx,_,didx) in dep_arcs if gidx == nidx] ) \n",
    "            return ref_edges.issubset(pred_edges) \n",
    "        \n",
    "        S,B,dep_arcs = config\n",
    "        if len(S) >= 2:\n",
    "            e = deptree.get_edge(S[-2],S[-1])\n",
    "            if e and node_is_complete(S[-1],deptree,dep_arcs):\n",
    "                gidx,deplabel,deptag,didx = e[0]\n",
    "                return (ArcStandardParser.RARC,deplabel)\n",
    "            \n",
    "            e = deptree.get_edge(S[-1],S[-2]) \n",
    "            if S[-2] != 0 and e and node_is_complete(S[-2],deptree,dep_arcs):\n",
    "                gidx,deplabel,deptag,didx = e[0]\n",
    "                return (ArcStandardParser.LARC,deplabel)\n",
    "        if B: \n",
    "            return (ArcStandardParser.SHIFT,None)\n",
    "    \n",
    "        return None #terminate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_parser(model,test_treebank,display=False):\n",
    "    \n",
    "    las_correct = 0\n",
    "    uas_correct = 0\n",
    "    tag_correct = 0\n",
    "    \n",
    "    for dtree in test_treebank:\n",
    "        \n",
    "        X           = dtree.word_list()\n",
    "        ptree       = model.predict_tree(X)\n",
    "        \n",
    "        ref_edges   = set( [(gidx,deplabel,didx) for (gidx,deplabel,_,didx) in dtree.edges ] )\n",
    "        pred_edges  = set( [(gidx,deplabel,didx) for (gidx,deplabel,_,didx) in ptree.edges ] )\n",
    "        las_correct+= len(ref_edges.intersection(pred_edges))/len(ref_edges)\n",
    "        \n",
    "        ref_edges   = set( [(gidx,didx) for (gidx,deplabel,_,didx) in dtree.edges ] )\n",
    "        pred_edges  = set( [(gidx,didx) for (gidx,deplabel,_,didx) in ptree.edges ] )\n",
    "        uas_correct+= len(ref_edges.intersection(pred_edges))/len(ref_edges)\n",
    "        \n",
    "        ref_tags     = dtree.tag_list()\n",
    "        pred_tags    = ptree.tag_list()\n",
    "        tag_correct += sum([p==r for p,r in zip(pred_tags,ref_tags)])/len(ref_edges)\n",
    "        \n",
    "        if display:\n",
    "            print(ptree,'\\n')\n",
    "    \n",
    "    print('Model accurracy LAS: %f ; UAS: %f ; Tag accuracy: %f'%(las_correct/len(test_treebank),uas_correct/len(test_treebank),tag_correct/len(test_treebank)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  5.52it/s]\n",
      "\n",
      "[train]      Epoch 0, NLL = 5.219214\n",
      "[validation] Epoch 0, NLL = 4.255364\n",
      "\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.39it/s]\n",
      "\n",
      "[train]      Epoch 1, NLL = 4.136380\n",
      "[validation] Epoch 1, NLL = 3.948225\n",
      "\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.45it/s]\n",
      "\n",
      "[train]      Epoch 2, NLL = 3.815045\n",
      "[validation] Epoch 2, NLL = 3.579101\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  5.14it/s]\n",
      "\n",
      "[train]      Epoch 3, NLL = 3.411591\n",
      "[validation] Epoch 3, NLL = 3.147165\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.99it/s]\n",
      "\n",
      "[train]      Epoch 4, NLL = 3.087611\n",
      "[validation] Epoch 4, NLL = 2.798483\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.89it/s]\n",
      "\n",
      "[train]      Epoch 5, NLL = 2.740148\n",
      "[validation] Epoch 5, NLL = 2.531080\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.99it/s]\n",
      "\n",
      "[train]      Epoch 6, NLL = 2.489665\n",
      "[validation] Epoch 6, NLL = 2.301267\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.85it/s]\n",
      "\n",
      "[train]      Epoch 7, NLL = 2.273573\n",
      "[validation] Epoch 7, NLL = 2.090828\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.79it/s]\n",
      "\n",
      "[train]      Epoch 8, NLL = 2.086163\n",
      "[validation] Epoch 8, NLL = 1.904639\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "\n",
      "[train]      Epoch 9, NLL = 1.924262\n",
      "[validation] Epoch 9, NLL = 1.737043\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.86it/s]\n",
      "\n",
      "[train]      Epoch 10, NLL = 1.710443\n",
      "[validation] Epoch 10, NLL = 1.569481\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.97it/s]\n",
      "\n",
      "[train]      Epoch 11, NLL = 1.620536\n",
      "[validation] Epoch 11, NLL = 1.419081\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.81it/s]\n",
      "\n",
      "[train]      Epoch 12, NLL = 1.510661\n",
      "[validation] Epoch 12, NLL = 1.317810\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.96it/s]\n",
      "\n",
      "[train]      Epoch 13, NLL = 1.386454\n",
      "[validation] Epoch 13, NLL = 1.192826\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  5.03it/s]\n",
      "\n",
      "[train]      Epoch 14, NLL = 1.289336\n",
      "[validation] Epoch 14, NLL = 1.154597\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.91it/s]\n",
      "\n",
      "[train]      Epoch 15, NLL = 1.235640\n",
      "[validation] Epoch 15, NLL = 1.102652\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.62it/s]\n",
      "\n",
      "[train]      Epoch 16, NLL = 1.142425\n",
      "[validation] Epoch 16, NLL = 0.993638\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.67it/s]\n",
      "\n",
      "[train]      Epoch 17, NLL = 1.035008\n",
      "[validation] Epoch 17, NLL = 0.870397\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "\n",
      "[train]      Epoch 18, NLL = 1.032137\n",
      "[validation] Epoch 18, NLL = 0.802562\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.69it/s]\n",
      "\n",
      "[train]      Epoch 19, NLL = 0.932958\n",
      "[validation] Epoch 19, NLL = 0.746787\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.61it/s]\n",
      "\n",
      "[train]      Epoch 20, NLL = 0.888185\n",
      "[validation] Epoch 20, NLL = 0.717125\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.87it/s]\n",
      "\n",
      "[train]      Epoch 21, NLL = 0.813496\n",
      "[validation] Epoch 21, NLL = 0.619797\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.48it/s]\n",
      "\n",
      "[train]      Epoch 22, NLL = 0.729120\n",
      "[validation] Epoch 22, NLL = 0.611875\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.66it/s]\n",
      "\n",
      "[train]      Epoch 23, NLL = 0.792817\n",
      "[validation] Epoch 23, NLL = 0.542529\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.42it/s]\n",
      "\n",
      "[train]      Epoch 24, NLL = 0.651409\n",
      "[validation] Epoch 24, NLL = 0.524746\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.71it/s]\n",
      "\n",
      "[train]      Epoch 25, NLL = 0.631363\n",
      "[validation] Epoch 25, NLL = 0.479125\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.70it/s]\n",
      "\n",
      "[train]      Epoch 26, NLL = 0.593127\n",
      "[validation] Epoch 26, NLL = 0.464331\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.45it/s]\n",
      "\n",
      "[train]      Epoch 27, NLL = 0.617028\n",
      "[validation] Epoch 27, NLL = 0.507535\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.47it/s]\n",
      "\n",
      "[train]      Epoch 28, NLL = 0.554806\n",
      "[validation] Epoch 28, NLL = 0.470670\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.37it/s]\n",
      "\n",
      "[train]      Epoch 29, NLL = 0.564594\n",
      "[validation] Epoch 29, NLL = 0.534123\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.41it/s]\n",
      "\n",
      "[train]      Epoch 30, NLL = 0.524751\n",
      "[validation] Epoch 30, NLL = 0.488098\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.76it/s]\n",
      "\n",
      "[train]      Epoch 31, NLL = 0.556662\n",
      "[validation] Epoch 31, NLL = 0.452039\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "\n",
      "[train]      Epoch 32, NLL = 0.516560\n",
      "[validation] Epoch 32, NLL = 0.413143\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.77it/s]\n",
      "\n",
      "[train]      Epoch 33, NLL = 0.462543\n",
      "[validation] Epoch 33, NLL = 0.319924\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.59it/s]\n",
      "\n",
      "[train]      Epoch 34, NLL = 0.388869\n",
      "[validation] Epoch 34, NLL = 0.263184\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.85it/s]\n",
      "\n",
      "[train]      Epoch 35, NLL = 0.356020\n",
      "[validation] Epoch 35, NLL = 0.266383\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.76it/s]\n",
      "\n",
      "[train]      Epoch 36, NLL = 0.380876\n",
      "[validation] Epoch 36, NLL = 0.278424\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.93it/s]\n",
      "\n",
      "[train]      Epoch 37, NLL = 0.309689\n",
      "[validation] Epoch 37, NLL = 0.222642\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.65it/s]\n",
      "\n",
      "[train]      Epoch 38, NLL = 0.282800\n",
      "[validation] Epoch 38, NLL = 0.181051\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.55it/s]\n",
      "\n",
      "[train]      Epoch 39, NLL = 0.238702\n",
      "[validation] Epoch 39, NLL = 0.173083\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.66it/s]\n",
      "\n",
      "[train]      Epoch 40, NLL = 0.233736\n",
      "[validation] Epoch 40, NLL = 0.160828\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.60it/s]\n",
      "\n",
      "[train]      Epoch 41, NLL = 0.268225\n",
      "[validation] Epoch 41, NLL = 0.148563\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.24it/s]\n",
      "\n",
      "[train]      Epoch 42, NLL = 0.207073\n",
      "[validation] Epoch 42, NLL = 0.134847\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.45it/s]\n",
      "\n",
      "[train]      Epoch 43, NLL = 0.238228\n",
      "[validation] Epoch 43, NLL = 0.125018\n",
      "\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.06it/s]\n",
      "\n",
      "[train]      Epoch 44, NLL = 0.192543\n",
      "[validation] Epoch 44, NLL = 0.122782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = ArcStandardParser(32,64,100)\n",
    "m.train_model(valid_treebank[:20],valid_treebank[:20],epochs=45,learning_rate=0.001,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tNous\tCL\tsuj\t2\n",
      "2\tprions\tV\troot\t0\n",
      "3\tles\tD\tdet\t4\n",
      "4\tcinéastes\tN\tobj\t2\n",
      "5\tet\tC\tcoord\t4\n",
      "6\ttous\tA\tmod\t8\n",
      "7\tnos\tD\tdet\t8\n",
      "8\tlecteurs\tN\tdep.coord\t5\n",
      "9\tde\tP\tp_obj\t2\n",
      "10\tbien\tADV\tmod\t11\n",
      "11\tvouloir\tV\tobj.p\t9\n",
      "12\tnous\tCL\tde_obj\t14\n",
      "13\ten\tCL\tobj\t14\n",
      "14\texcuser\tV\tobj\t11\n",
      "15\t.\tPONCT\tponct\t2 \n",
      "\n",
      "1\tLa\tD\tdet\t2\n",
      "2\tdiffusion\tA\tsuj\t8\n",
      "3\tdes\tP+D\tdep\t2\n",
      "4\tprévisions\tN\tobj.p\t3\n",
      "5\tmétéorologiques\tA\tdep_cpd\t4\n",
      "6\tétait\tV\taff\t8\n",
      "7\tfortement\tADV\tmod\t8\n",
      "8\tperturbée\tV\troot\t0\n",
      "9\t,\tPONCT\tponct\t8\n",
      "10\tmardi\tN\tmod\t8\n",
      "11\t7\tA\tmod\t10\n",
      "12\tjanvier\tN\tmod\t10\n",
      "13\t,\tPONCT\tponct\t8\n",
      "14\tpar\tP\tmod\t8\n",
      "15\tle\tD\tdet\t16\n",
      "16\tmouvement\tN\tobj.p\t14\n",
      "17\tde\tP\tdep\t16\n",
      "18\tgrève\tN\tobj.p\t17\n",
      "19\tnationale\tA\tmod\t18\n",
      "20\tde\tP\tdep\t16\n",
      "21\ttrente\tD\tdet\t24\n",
      "22\t-\tPONCT\tdep_cpd\t21\n",
      "23\tsix\tD\tdep_cpd\t21\n",
      "24\theures\tN\tobj.p\t20\n",
      "25\tdéclenché\tV\tdep\t16\n",
      "26\tla\tD\tdet\t29\n",
      "27\tveille\tN\tdep_cpd\t26\n",
      "28\tau\tP+D\tdep_cpd\t26\n",
      "29\tsoir\tN\tmod\t25\n",
      "30\t,\tPONCT\tponct\t29\n",
      "31\tà\tP\tdep\t29\n",
      "32\tl'\tD\tdep_cpd\t31\n",
      "33\tappel\tN\tdep_cpd\t31\n",
      "34\tdes\tP+D\tdep\t29\n",
      "35\tdélégués\tN\tobj.p\t34\n",
      "36\tCGT\tN\tmod\t35\n",
      "37\t,\tPONCT\tcoord\t36\n",
      "38\tCFDT\tN\tdep.coord\t37\n",
      "39\tet\tC\tcoord\t36\n",
      "40\tFO\tN\tdep.coord\t39\n",
      "41\tdu\tP+D\tdep\t29\n",
      "42\tpersonnel\tN\tobj.p\t41\n",
      "43\ttechnique\tA\tdep_cpd\t42\n",
      "44\tde\tP\tdep\t42\n",
      "45\tMétéo\tN\tobj.p\t44\n",
      "46\t-\tPONCT\tdep_cpd\t45\n",
      "47\tFrance\tN\tdep_cpd\t45\n",
      "48\t.\tPONCT\tponct\t8 \n",
      "\n",
      "1\tPar\tP\tmod\t7\n",
      "2\tcette\tD\tdet\t3\n",
      "3\taction\tN\tobj.p\t1\n",
      "4\t,\tPONCT\tponct\t7\n",
      "5\tces\tD\tdet\t6\n",
      "6\tderniers\tA\tsuj\t7\n",
      "7\tveulent\tV\troot\t0\n",
      "8\tdénoncer\tV\tobj\t7\n",
      "9\tla\tD\tdet\t10\n",
      "10\tbaisse\tN\tobj\t8\n",
      "11\tdes\tP+D\tdep\t10\n",
      "12\teffectifs\tN\tobj.p\t11\n",
      "13\tqui\tPRO\tdep\t10\n",
      "14\tse\tCL\taff\t15\n",
      "15\ttraduit\tV\tdep\t13\n",
      "16\t,\tPONCT\tponct\t17\n",
      "17\taffirment\tV\tmod\t15\n",
      "18\t-ils\tCL\tsuj\t17\n",
      "19\t,\tPONCT\tponct\t17\n",
      "20\tpar\tP\tponct\t17\n",
      "21\t\"\tPONCT\tponct\t20\n",
      "22\tl'\tD\tponct\t17\n",
      "23\tabandon\tN\tmod\t17\n",
      "24\tde\tP\tmod\t15\n",
      "25\tcertaines\tD\tdet\t26\n",
      "26\ttâches\tN\tobj.p\t24\n",
      "27\tet\tC\tcoord\t15\n",
      "28\tle\tD\tdet\t29\n",
      "29\trecours\tN\tdep.coord\t27\n",
      "30\tà\tP\tdep\t29\n",
      "31\tla\tD\tdet\t32\n",
      "32\tsous\tADV\tobj.p\t30\n",
      "33\t-\tPONCT\tdep_cpd\t32\n",
      "34\ttraitance\tN\tdep_cpd\t32\n",
      "35\t\"\tPONCT\tponct\t7\n",
      "36\t.\tPONCT\tponct\t7 \n",
      "\n",
      "1\tLes\tD\tdet\t2\n",
      "2\tsyndicats\tN\tsuj\t3\n",
      "3\tprotestent\tV\troot\t0\n",
      "4\taussi\tADV\tmod\t3\n",
      "5\tcontre\tP\tmod\t3\n",
      "6\tle\tD\tdet\t7\n",
      "7\tblocage\tN\tobj.p\t5\n",
      "8\t,\tPONCT\tponct\t3\n",
      "9\t\"\tPONCT\tponct\t3\n",
      "10\tdepuis\tP\tmod\t3\n",
      "11\tdeux\tD\tdet\t12\n",
      "12\tans\tN\tobj.p\t10\n",
      "13\t\"\tPONCT\tponct\t3\n",
      "14\t,\tPONCT\tponct\t3\n",
      "15\tdes\tP+D\tdep\t3\n",
      "16\trevendications\tN\tobj.p\t15\n",
      "17\tstatutaires\tA\tmod\t16\n",
      "18\tde\tP\tdep\t16\n",
      "19\ttous\tA\tmod\t21\n",
      "20\tles\tD\tdet\t21\n",
      "21\tcorps\tN\tobj.p\t18\n",
      "22\tdu\tP+D\tdep\t21\n",
      "23\tpersonnel\tN\tobj.p\t22\n",
      "24\tet\tC\tponct\t25\n",
      "25\tréclament\tV\tmod\t3\n",
      "26\tl'\tD\tdet\t27\n",
      "27\touverture\tN\tobj\t25\n",
      "28\tde\tP\tdep\t27\n",
      "29\tnégociations\tN\tobj.p\t28\n",
      "30\tainsi\tADV\tcoord\t27\n",
      "31\tque\tC\tdep_cpd\t30\n",
      "32\tla\tD\tdet\t33\n",
      "33\tcréation\tN\tdep\t30\n",
      "34\tde\tP\tdep\t33\n",
      "35\tpostes\tN\tobj.p\t34\n",
      "36\ttechniques\tA\tmod\t35\n",
      "37\tet\tC\tcoord\t36\n",
      "38\tadministratifs\tA\tdep.coord\t37\n",
      "39\t.\tPONCT\tponct\t3 \n",
      "\n",
      "1\t-LSB-\tPONCT\tponct\t5\n",
      "2\tCe\tD\tdet\t3\n",
      "3\tmouvement\tN\tsuj\t5\n",
      "4\tnous\tCL\tobj\t5\n",
      "5\tempêche\tV\troot\t0\n",
      "6\tde\tP\tobj\t5\n",
      "7\tpublier\tV\tobj.p\t6\n",
      "8\tles\tD\tdet\t9\n",
      "9\tprévisions\tN\tobj\t7\n",
      "10\thabituelles\tA\tmod\t9\n",
      "11\t.\tPONCT\tponct\t5 \n",
      "\n",
      "1\tNous\tCL\tsuj\t2\n",
      "2\tprions\tV\troot\t0\n",
      "3\tnos\tD\tdet\t4\n",
      "4\tlecteurs\tN\tobj\t2\n",
      "5\tde\tP\tp_obj\t2\n",
      "6\tbien\tADV\tmod\t7\n",
      "7\tvouloir\tV\tobj.p\t5\n",
      "8\tnous\tCL\tobj\t9\n",
      "9\texcuser\tV\tobj\t7\n",
      "10\t.\tPONCT\tponct\t2\n",
      "11\t-RSB-\tPONCT\tponct\t2 \n",
      "\n",
      "1\tThe\tET\tmod\t5\n",
      "2\tEuropean\tET\tmod\t5\n",
      "3\t,\tPONCT\tponct\t5\n",
      "4\tl'\tD\tdet\t5\n",
      "5\thebdomadaire\tA\tsuj\t21\n",
      "6\tde\tP\tdep\t5\n",
      "7\tlangue\tN\tobj.p\t6\n",
      "8\tanglaise\tA\tmod\t7\n",
      "9\tcréé\tV\tmod\t5\n",
      "10\tpar\tP\tp_obj\t9\n",
      "11\tRobert\tN\tobj.p\t10\n",
      "12\tMaxwell\tN\tmod\t11\n",
      "13\tà\tP\tmod\t9\n",
      "14\tdestination\tN\tdep_cpd\t13\n",
      "15\tdu\tP+D\tdep_cpd\t13\n",
      "16\tpublic\tN\tobj.p\t13\n",
      "17\teuropéen\tA\tmod\t16\n",
      "18\t,\tPONCT\tponct\t21\n",
      "19\ta\tV\taux.tps\t21\n",
      "20\tété\tV\taux.tps\t21\n",
      "21\tvendu\tV\troot\t0\n",
      "22\tà\tP\ta_obj\t21\n",
      "23\tune\tD\tdet\t24\n",
      "24\tsociété\tN\tobj.p\t22\n",
      "25\tcontrôlée\tV\tmod\t24\n",
      "26\tpar\tP\tdep\t25\n",
      "27\tdeux\tD\tdet\t28\n",
      "28\tfinanciers\tN\tobj.p\t26\n",
      "29\td'\tP\tdep\t28\n",
      "30\torigine\tN\tobj.p\t29\n",
      "31\técossaise\tA\tmod\t30\n",
      "32\t,\tPONCT\tponct\t28\n",
      "33\tles\tD\tdet\t34\n",
      "34\tjumeaux\tN\tmod\t28\n",
      "35\tDavid\tN\tmod\t34\n",
      "36\tet\tC\tcoord\t35\n",
      "37\tFrederick\tN\tdep.coord\t36\n",
      "38\tBarclay\tN\tmod\t35\n",
      "39\t.\tPONCT\tponct\t21 \n",
      "\n",
      "1\tLe\tD\tdet\t2\n",
      "2\tprix\tN\tsuj\t11\n",
      "3\tde\tP\tdep\t2\n",
      "4\tvente\tN\tobj.p\t3\n",
      "5\tdu\tP+D\tdep\t4\n",
      "6\tjournal\tN\tobj.p\t5\n",
      "7\tn'\tADV\tmod\t11\n",
      "8\ta\tV\taux.tps\t11\n",
      "9\tpas\tADV\tmod\t11\n",
      "10\tété\tV\taux.pass\t11\n",
      "11\tdivulgué\tV\troot\t0\n",
      "12\t.\tPONCT\tponct\t11 \n",
      "\n",
      "1\tLe\tD\tdet\t2\n",
      "2\tcabinet\tN\tsuj\t22\n",
      "3\tArthur\tN\tmod\t2\n",
      "4\tAndersen\tN\tmod\t2\n",
      "5\t,\tPONCT\tponct\t2\n",
      "6\tadministrateur\tN\tmod\t2\n",
      "7\tjudiciaire\tA\tdep_cpd\t6\n",
      "8\tdu\tP+D\tdep\t6\n",
      "9\ttitre\tN\tobj.p\t8\n",
      "10\tdepuis\tP\tdep\t6\n",
      "11\tl'\tD\tdet\t12\n",
      "12\teffondrement\tN\tobj.p\t10\n",
      "13\tde\tP\tdep\t12\n",
      "14\tl'\tD\tdet\t15\n",
      "15\tempire\tN\tobj.p\t13\n",
      "16\tMaxwell\tN\tmod\t15\n",
      "17\ten\tP\tdep\t2\n",
      "18\tdécembre\tN\tobj.p\t17\n",
      "19\t1991\tN\tmod\t18\n",
      "20\t,\tPONCT\tponct\t22\n",
      "21\ta\tV\taux.tps\t22\n",
      "22\tindiqué\tV\troot\t0\n",
      "23\tque\tC\tobj\t22\n",
      "24\tles\tD\tdet\t26\n",
      "25\tnouveaux\tA\tmod\t26\n",
      "26\tpropriétaires\tN\tobj.cpl\t23\n",
      "27\tallaient\tV\tmod\t26\n",
      "28\tpoursuivre\tV\tobj\t27\n",
      "29\tla\tD\tdet\t30\n",
      "30\tpublication\tN\tobj\t28\n",
      "31\t.\tPONCT\tponct\t22 \n",
      "\n",
      "1\t-\tPONCT\tponct\t11\n",
      "2\tDepuis\tP\tmod\t11\n",
      "3\tle\tD\tdet\t5\n",
      "4\t1\tA\tmod\t5\n",
      "5\tjanvier\tN\tobj.p\t2\n",
      "6\t,\tPONCT\tponct\t11\n",
      "7\tla\tD\tdet\t8\n",
      "8\tcompagnie\tN\tsuj\t11\n",
      "9\tAir\tN\tmod\t8\n",
      "10\tInter\tA\tdep_cpd\t9\n",
      "11\tapplique\tV\troot\t0\n",
      "12\tune\tD\tdet\t13\n",
      "13\thausse\tN\tobj\t11\n",
      "14\tmoyenne\tA\tmod\t13\n",
      "15\tde\tP\tdep\t13\n",
      "16\tses\tD\tdet\t17\n",
      "17\ttarifs\tN\tobj.p\t15\n",
      "18\tde\tP\tdep\t13\n",
      "19\t3\tD\tdet\t22\n",
      "20\t,\tPONCT\tdep_cpd\t19\n",
      "21\t3\tD\tdep_cpd\t19\n",
      "22\t%\tN\tobj.p\t18\n",
      "23\t,\tPONCT\tponct\t13\n",
      "24\tmodulée\tV\tmod\t13\n",
      "25\tselon\tP\tmod\t24\n",
      "26\tles\tD\tdet\t27\n",
      "27\tlignes\tN\tobj.p\t25\n",
      "28\t.\tPONCT\tponct\t11 \n",
      "\n",
      "1\tCette\tD\tdet\t2\n",
      "2\taugmentation\tN\tsuj\t5\n",
      "3\ttarifaire\tA\tdep_cpd\t2\n",
      "4\ts'\tCL\taff\t5\n",
      "5\tinscrit\tV\troot\t0\n",
      "6\tdans\tP\tp_obj\t5\n",
      "7\tune\tD\tdet\t8\n",
      "8\tfourchette\tN\tobj.p\t6\n",
      "9\tde\tP\tdep\t8\n",
      "10\t0\tD\tdet\t13\n",
      "11\t,\tPONCT\tdep_cpd\t10\n",
      "12\t3\tD\tdep_cpd\t10\n",
      "13\t%\tN\tobj.p\t9\n",
      "14\tà\tP\targ\t9\n",
      "15\t4\tD\tdet\t18\n",
      "16\t,\tPONCT\tdep_cpd\t15\n",
      "17\t9\tD\tdep_cpd\t15\n",
      "18\t%\tN\tobj.p\t14\n",
      "19\t.\tPONCT\tponct\t5 \n",
      "\n",
      "1\tToutefois\tADV\tmod\t6\n",
      "2\t,\tPONCT\tponct\t6\n",
      "3\tles\tD\tdet\t4\n",
      "4\ttarifs\tN\tsuj\t6\n",
      "5\tréduits\tA\tmod\t4\n",
      "6\tprogresseront\tV\troot\t0\n",
      "7\tmoins\tADV\tmod\t6\n",
      "8\tque\tC\tdep\t7\n",
      "9\tl'\tD\tdet\t10\n",
      "10\tinflation\tN\tobj.cpl\t8\n",
      "11\tet\tC\tcoord\t6\n",
      "12\tmême\tC\tdep_cpd\t13\n",
      "13\tdiminueront\tV\tdep\t11\n",
      "14\t,\tPONCT\tponct\t13\n",
      "15\tpour\tP\tmod\t13\n",
      "16\tles\tD\tdet\t17\n",
      "17\tjeunes\tA\tobj.p\t15\n",
      "18\t,\tPONCT\tponct\t13\n",
      "19\tsur\tP\tmod\t13\n",
      "20\tcertaines\tD\tdet\t21\n",
      "21\tlignes\tN\tobj.p\t19\n",
      "22\tcomme\tP\tdep\t21\n",
      "23\tParis\tN\tobj.p\t22\n",
      "24\t-\tPONCT\tcoord\t23\n",
      "25\tLyon\tN\tdep.coord\t24\n",
      "26\t,\tPONCT\tcoord\t23\n",
      "27\tParis\tN\tdep.coord\t26\n",
      "28\t-\tPONCT\tponct\t27\n",
      "29\tNantes\tN\tmod\t27\n",
      "30\tou\tC\tcoord\t23\n",
      "31\tParis\tN\tdep.coord\t30\n",
      "32\t-\tPONCT\tponct\t31\n",
      "33\tBordeaux\tN\tmod\t31\n",
      "34\t.\tPONCT\tponct\t6 \n",
      "\n",
      "1\tLa\tD\tdet\t3\n",
      "2\tdernière\tA\tmod\t3\n",
      "3\taugmentation\tN\tsuj\t10\n",
      "4\tdes\tP+D\tdep\t3\n",
      "5\ttarifs\tN\tobj.p\t4\n",
      "6\td'\tP\tdep\t5\n",
      "7\tAir\tN\tobj.p\t6\n",
      "8\tInter\tA\tdep_cpd\t7\n",
      "9\tavait\tV\taux.tps\t10\n",
      "10\tété\tV\troot\t0\n",
      "11\tde\tP\tats\t10\n",
      "12\t5\tD\tdet\t13\n",
      "13\t%\tN\tobj.p\t11\n",
      "14\ten\tP\tmod\t10\n",
      "15\tjanvier\tN\tobj.p\t14\n",
      "16\t1991\tN\tmod\t15\n",
      "17\t.\tPONCT\tponct\t10 \n",
      "\n",
      "1\tLes\tD\tdet\t2\n",
      "2\tventes\tN\tsuj\t10\n",
      "3\tde\tP\tdep\t2\n",
      "4\tmicro-\tPREF\tmod\t5\n",
      "5\tordinateurs\tN\tobj.p\t3\n",
      "6\ten\tP\tmod\t10\n",
      "7\tFrance\tN\tobj.p\t6\n",
      "8\tse\tCL\taff\t10\n",
      "9\tsont\tV\taux.tps\t10\n",
      "10\tralenties\tV\troot\t0\n",
      "11\ten\tP\tmod\t10\n",
      "12\t1991\tN\tobj.p\t11\n",
      "13\t.\tPONCT\tponct\t10 \n",
      "\n",
      "1\tLeur\tD\tdet\t2\n",
      "2\tprogression\tN\tsuj\t15\n",
      "3\t,\tPONCT\tponct\t2\n",
      "4\tqui\tPRO\tmod\t2\n",
      "5\tétait\tV\tmod.rel\t4\n",
      "6\tencore\tADV\tats\t5\n",
      "7\tde\tP\tats\t5\n",
      "8\t13\tD\tdet\t9\n",
      "9\t%\tN\tobj.p\t7\n",
      "10\ten\tP\tmod\t5\n",
      "11\t1990\tN\tobj.p\t10\n",
      "12\t,\tPONCT\tponct\t15\n",
      "13\ts'\tCL\taff\t15\n",
      "14\test\tV\taux.tps\t15\n",
      "15\tétablie\tV\troot\t0\n",
      "16\tà\tP\tats\t15\n",
      "17\t9\tD\tdet\t18\n",
      "18\t%\tN\tobj.p\t16\n",
      "19\tl'\tD\tdet\t20\n",
      "20\tan\tN\tmod\t15\n",
      "21\tpassé\tA\tmod\t20\n",
      "22\t,\tPONCT\tponct\t23\n",
      "23\tindique\tV\tmod\t15\n",
      "24\tl'\tD\tdet\t25\n",
      "25\tinstitut\tN\tsuj\t23\n",
      "26\tnational\tA\tdep_cpd\t25\n",
      "27\tde\tP\tdep_cpd\t25\n",
      "28\tla\tD\tdep_cpd\t25\n",
      "29\tstatistique\tN\tdep_cpd\t25\n",
      "30\tet\tC\tdep_cpd\t25\n",
      "31\tdes\tP+D\tdep_cpd\t25\n",
      "32\tétudes\tN\tdep_cpd\t25\n",
      "33\téconomiques\tA\tdep_cpd\t25\n",
      "34\t,\tPONCT\tponct\t15\n",
      "35\tdans\tP\tmod\t15\n",
      "36\tune\tD\tdet\t37\n",
      "37\tétude\tN\tobj.p\t35\n",
      "38\trendue\tV\tmod\t37\n",
      "39\tpublique\tA\tmod\t38\n",
      "40\tmardi\tN\tmod\t39\n",
      "41\t7\tA\tmod\t40\n",
      "42\tjanvier\tN\tmod\t39\n",
      "43\t.\tPONCT\tponct\t15 \n",
      "\n",
      "1\tLe\tD\tdet\t2\n",
      "2\tmarché\tN\tsuj\t14\n",
      "3\tde\tP\tdep\t2\n",
      "4\tla\tD\tdet\t6\n",
      "5\tmicro-\tPREF\tmod\t6\n",
      "6\tinformatique\tN\tobj.p\t3\n",
      "7\t,\tPONCT\tponct\t8\n",
      "8\tnotent\tV\tmod\t14\n",
      "9\tles\tD\tdet\t10\n",
      "10\texperts\tN\tsuj\t8\n",
      "11\t,\tPONCT\tponct\t8\n",
      "12\ta\tV\taux.tps\t14\n",
      "13\tainsi\tADV\taux.tps\t14\n",
      "14\tabandonné\tV\troot\t0\n",
      "15\ten\tP\tmod\t14\n",
      "16\t1991\tN\tobj.p\t15\n",
      "17\tles\tD\tdet\t18\n",
      "18\ttaux\tN\tobj\t14\n",
      "19\tde\tP\tdep\t18\n",
      "20\tcroissance\tN\tobj.p\t19\n",
      "21\tà\tP\tdep\t18\n",
      "22\tdeux\tD\tdet\t23\n",
      "23\tchiffres\tN\tobj.p\t21\n",
      "24\tdont\tPRO\tmod\t26\n",
      "25\til\tCL\tmod\t26\n",
      "26\tétait\tV\tmod.rel\t18\n",
      "27\tfamilier\tA\tats\t26\n",
      "28\tdepuis\tP\tmod\t26\n",
      "29\tdes\tD\tdet\t30\n",
      "30\tannées\tN\tobj.p\t28\n",
      "31\t.\tPONCT\tponct\t14 \n",
      "\n",
      "1\tDe\tP\tmod\t12\n",
      "2\t1986\tN\tobj.p\t1\n",
      "3\tà\tP\targ\t1\n",
      "4\t1989\tN\tobj.p\t3\n",
      "5\t,\tPONCT\tponct\t12\n",
      "6\tle\tD\tdet\t7\n",
      "7\tnombre\tN\tsuj\t12\n",
      "8\td'\tP\tdep\t7\n",
      "9\tunités\tN\tobj.p\t8\n",
      "10\tvendues\tV\tmod\t9\n",
      "11\tavait\tV\taux.tps\t12\n",
      "12\taugmenté\tV\troot\t0\n",
      "13\ten\tP\tmod\t12\n",
      "14\tmoyenne\tN\tdep_cpd\t13\n",
      "15\tde\tP\tde_obj\t12\n",
      "16\t26\tD\tdet\t17\n",
      "17\t%\tN\tobj.p\t15\n",
      "18\tpar\tP\tdep\t17\n",
      "19\tan\tN\tobj.p\t18\n",
      "20\t.\tPONCT\tponct\t12 \n",
      "\n",
      "1\tA\tP\tmod\t9\n",
      "2\tla\tD\tdet\t3\n",
      "3\tpériode\tN\tobj.p\t1\n",
      "4\td'\tP\tdep\t3\n",
      "5\téquipement\tN\tobj.p\t4\n",
      "6\tet\tC\tcoord\t4\n",
      "7\tde\tP\tdep.coord\t6\n",
      "8\tdécollage\tN\tobj.p\t7\n",
      "9\tsemble\tV\troot\t0\n",
      "10\tdevoir\tV\tats\t9\n",
      "11\tsuccéder\tV\tobj\t10\n",
      "12\tune\tD\tdet\t13\n",
      "13\tphase\tN\tsuj\t9\n",
      "14\tde\tP\tdep\t13\n",
      "15\tplus\tADV\tmod\t16\n",
      "16\tgrande\tA\tmod\t17\n",
      "17\tmaturité\tN\tobj.p\t14\n",
      "18\tde\tP\tdep\t17\n",
      "19\tla\tD\tdet\t20\n",
      "20\tdemande\tN\tobj.p\t18\n",
      "21\t.\tPONCT\tponct\t9 \n",
      "\n",
      "1\tDéjà\tADV\tmod\t12\n",
      "2\t,\tPONCT\tponct\t12\n",
      "3\ten\tP\tmod\t12\n",
      "4\t1990\tN\tobj.p\t3\n",
      "5\t,\tPONCT\tponct\t12\n",
      "6\t40\tD\tdet\t7\n",
      "7\t%\tN\tsuj\t12\n",
      "8\tdes\tP+D\tdep\t7\n",
      "9\tunités\tN\tobj.p\t8\n",
      "10\tvendues\tV\tmod\t9\n",
      "11\tétaient\tV\taux.pass\t12\n",
      "12\tdestinées\tV\troot\t0\n",
      "13\tà\tP\ta_obj\t12\n",
      "14\trenouveler\tV\tobj.p\t13\n",
      "15\tle\tD\tdet\t16\n",
      "16\tmatériel\tN\tobj\t14\n",
      "17\texistant\tA\tmod\t16\n",
      "18\t,\tPONCT\tponct\t19\n",
      "19\tsouligne\tV\tmod\t12\n",
      "20\tl'\tD\tdet\t21\n",
      "21\tINSEE\tN\tsuj\t19\n",
      "22\tqui\tPRO\tsuj\t23\n",
      "23\tcite\tV\tmod.rel\t21\n",
      "24\tune\tD\tdet\t25\n",
      "25\tenquête\tN\tobj\t23\n",
      "26\tde\tP\tdep\t25\n",
      "27\tla\tD\tdet\t28\n",
      "28\tsociété\tN\tobj.p\t26\n",
      "29\td'\tP\tdep\t28\n",
      "30\tétudes\tN\tobj.p\t29\n",
      "31\tspécialisées\tA\tmod\t30\n",
      "32\tIDC\tN\tmod\t28\n",
      "33\t.\tPONCT\tponct\t12 \n",
      "\n",
      "1\tLes\tD\tdet\t2\n",
      "2\tconstructeurs\tN\tsuj\t4\n",
      "3\tont\tV\taux.tps\t4\n",
      "4\tréagi\tV\troot\t0\n",
      "5\tà\tP\ta_obj\t4\n",
      "6\tcette\tD\tdet\t7\n",
      "7\tphase\tN\tobj.p\t5\n",
      "8\tde\tP\tdep\t7\n",
      "9\tralentissement\tN\tobj.p\t8\n",
      "10\tdes\tP+D\tdep\t9\n",
      "11\tventes\tN\tobj.p\t10\n",
      "12\ten\tP\tmod\t4\n",
      "13\treprenant\tV\tobj.p\t12\n",
      "14\tl'\tD\tdet\t15\n",
      "15\toffensive\tN\tobj\t13\n",
      "16\t.\tPONCT\tponct\t4 \n",
      "\n",
      "Model accurracy LAS: 0.936689 ; UAS: 0.955747 ; Tag accuracy: 0.998958\n"
     ]
    }
   ],
   "source": [
    "eval_parser(m,valid_treebank[:20],display=True)\n",
    "#eval_parser(m,test_treebank,display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_parser(m,train_treebank,display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
