{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enoncé et consignes\n",
    "===================\n",
    "\n",
    "Le propos de ce projet est de finaliser l'implantation d'un parser en dépendances par transitions à partir d'une implémentation partielle limitée au tagging qui est proposée ci-dessous. \n",
    "\n",
    "Ce parser s'appuie sur un modèle d'analyse neuronal où la phrase d'entrée est encodée par deux bi-lstm, ce qui servira de features pour prendre les décisions de transition. Le modèle statistique de prédiction structurée sera un classifieur local.\n",
    "\n",
    "Le parser utilisera l'algorithme arc-standard et pourra utiliser une méthode de recherche de solutions gloutonne.\n",
    "\n",
    "Le jeu de données utilisé est une verions du corpus sequoia : \n",
    "    https://gforge.inria.fr/frs/?view=shownotes&group_id=3597&release_id=9064.\n",
    "qui a la propriété d'être projectif. \n",
    "Le fichier de données utilisé est `sequoia-corpus.np_conll`\n",
    "\n",
    "\n",
    "Objectifs\n",
    "--------\n",
    "Le parser à réaliser sera un hybride entre celui proposé par Kipperwasser et Goldberg (2016)\n",
    "et le système multitâche pour l'analyse en constituants décrit par Coavoux et Crabbé (2017).\n",
    "\n",
    "Soit $\\mathbf{a} = a_1\\ldots a_T$ une séquence de dérivation extraite d'un treebank par un oracle, avec $a_i$ une action et $\\mathbf{w}=w_1\\ldots w_n$ la séquence de mots correspondante.\n",
    "\n",
    "La log-probabilité conditionnelle d'un arbre de dépendances se décompose par :\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\log P(\\mathbf{a} | \\mathbf{w};\\boldsymbol\\theta) =  \\sum_{i=1}^T \\log P(a_i| \\mathbf{w};\\boldsymbol\\theta ) \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Soit $\\mathbf{t} = t_1\\ldots t_n$ une séquence de tags correspondant à $w_1\\ldots w_n$, la log-probabilité conditionnelle de cette séquence de tags se décompose par :\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\log P(\\mathbf{t} | \\mathbf{w};\\boldsymbol\\theta) =  \\sum_{i=1}^n \\log P(t_i| \\mathbf{w};\\boldsymbol\\theta ) \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "L'apprentissage des paramètres consistera à maximiser la vraisemblance du treebank dans un contexte à deux objectifs : \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "{\\boldsymbol\\theta} = \\mathop{argmax}_{\\boldsymbol\\theta} \\sum_{j=1}^N \\\\\n",
    "\n",
    "\\log P(\\mathbf{a}_j | \\mathbf{w}_j;\\boldsymbol\\theta) + \\log P(\\mathbf{t}_j | \\mathbf{w}_j;\\boldsymbol\\theta) \n",
    "\n",
    "\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "\n",
    "Consignes spécifiques:\n",
    "---------------------\n",
    "\n",
    "1. Lire attentivement le code proposé ci-dessous et en discuter entre vous. \n",
    "2. Créer le système de transition :\n",
    "    * Modifier la méthode `index_symbols` pour qu'elle encode sur le modèle des actions shift les actions de type left arc right arc sous forme de couple à deux éléments associés à des entiers\n",
    "    * Implémenter les méthodes `shift` `left_arc`, `right_arc` \n",
    "    * Modifier la méthode `oracle` pour qu'elle implémente l'oracle statique arc-standard \n",
    "    * Implémenter la méthode `forward_parser`.\n",
    "3. Implémenter une méthode `predict_tree(...)` qui, à partir d'une configuration finale, renvoie l'arbre de dépendances prédit sous forme d'un objet `DependencyTree`.\n",
    "4. Implémenter une fonction `eval_parser` qui renvoie le LAS et l'UAS de votre parser.    \n",
    "5. Gestion des mots inconnus. Suite à vos premières évaluations, vous aurez peut-être remarqué que le tagger ou le parser a des performances relativement moyennes. Pour améliorer la situation, on propose d'incorporer un module de gestion des mots inconnus.\n",
    "    * Implanter une méthode `word_dropout(...,alpha=1.0)` qui prend en paramètre une liste de mots et qui renvoie cette liste avec certains mots remplacés par le token du mot inconnu. En général on choisit de remplacer un mot de basse fréquence $w$ par le token inconnu avec la probabilité $p = \\frac{\\alpha}{count(w)} $ où $\\alpha \\in [0,1]$ mais vous êtes libre de choisir votre méthode.  \n",
    "    * Ajputer un modèle de caractère au parser. Celui-ci prend en entrée une string et renvoie un vecteur calculé par un modèle RNN de caractères. Intégrer ce modèle à l'analyseur pour améliorer la gestion des mots inconnus (**Exercice plus compliqué**, mais vous pouvez vous inspirer de tutoriels existants sur le web)\n",
    "    * Evaluer l'impact de vos modifications et comparer par rapport à la baseline. Reporter vos UAS/LAS à chaque fois    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblio\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Kipperwasser et Goldberg, \"Simple and Accurate Dependency Parsing Using Bidirectional LSTM\" , _ACL_, 2016.\n",
    "*  Coavoux et Crabbé, \"Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks\", _EACL_ , 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import random\n",
    "\n",
    "from tqdm import tqdm #optional, for progress bars in notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure de données annexe\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyTree:\n",
    "\n",
    "    DUMMY_ROOT = '$ROOT$'\n",
    "    \n",
    "    def __init__(self,wordlist,edges=None):\n",
    "        \n",
    "            self.wordlist = wordlist                  #hyp: DUMMY ROOT is wordlist[0]  \n",
    "            self.edges    = edges if edges else []    #edges are 4-tuples (govidx,label,deptag,depidx)\n",
    "            self.edges.sort(key = lambda x:x[3])\n",
    "                \n",
    "    def tag_list(self):\n",
    "        return [deptag for (gidx,deplabel,deptag,didx) in self.edges]\n",
    "    def word_list(self):\n",
    "        return self.wordlist\n",
    "    def dlabel_list(self):\n",
    "         return [deplabel for (gidx,deplabel,deptag,didx) in self.edges]\n",
    "    \n",
    "    #aux functions useful for oracle\n",
    "    def get_edge(self,gov_idx,dep_idx):\n",
    "       #gets the edge (with labels) from gov_idx to dep_idx \n",
    "       #returns either a singleton or an empty list\n",
    "       return [ (gidx,x,y,didx) for (gidx,x,y,didx) in self.edges \\\n",
    "                                if gidx == gov_idx and didx == dep_idx]\n",
    "\n",
    "    def dom_edges(self,gov_idx):\n",
    "         #gets all edges couples from gov_idx to some dependant node\n",
    "         return [(gidx,didx) for (gidx,_,_,didx) in self.edges if gidx == gov_idx]\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_tree(istream):\n",
    "        \"\"\"\n",
    "        Reads a Conll-u tree and returns it as a DepTree object\n",
    "        returns None if no tree has been read\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        edges = []\n",
    "        for line in istream:\n",
    "            line = line.split(\"#\")[0]\n",
    "            if line and not line.isspace():\n",
    "                values = line.split()\n",
    "                words.append(values[1])\n",
    "                edges.append( (int(values[6]),values[7], values[3], int(values[0])) )\n",
    "            elif words:\n",
    "                return DependencyTree([DependencyTree.DUMMY_ROOT] + words,edges)\n",
    "        return DependencyTree([DependencyTree.DUMMY_ROOT] + words,edges) if words else None\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Pretty prints a tree for debug\n",
    "        Returns: a string\n",
    "        \"\"\"\n",
    "        lines =  [(didx,self.wordlist[didx],dtag,dlabel,gidx) for (gidx,dlabel,dtag,didx) in self.edges] \n",
    "        lines.sort(key=lambda x:x[0])\n",
    "        return '\\n'.join( [ \"%d\\t%s\\t%s\\t%s\\t%d\"%(L) for L in lines ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données\n",
    "===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_projective(root_idx,edges):\n",
    "    rspan = [root_idx]\n",
    "    children = [ didx for (gidx,_,_,didx) in edges if gidx==root_idx ]\n",
    "    for child in children:\n",
    "        proj,span = is_projective(child,edges)\n",
    "        if not proj:\n",
    "            return False,[]\n",
    "        rspan.extend(span)\n",
    "    rspan.sort()\n",
    "    return  ( all([ jdx == idx+1 for idx,jdx in zip(rspan,rspan[1:])]),rspan )\n",
    "    \n",
    "def read_trees(filename):\n",
    "\n",
    "    istream = open(filename)\n",
    "    \n",
    "    ilist = []\n",
    "    dtree = DependencyTree.read_tree(istream)\n",
    "    while dtree is not None:\n",
    "        proj,span = is_projective(0,dtree.edges)\n",
    "        if proj:\n",
    "            ilist.append(dtree)\n",
    "        #else:\n",
    "        #    print('[warning] non projective tree skipped')\n",
    "        dtree = DependencyTree.read_tree(istream)\n",
    "        \n",
    "    istream.close()\n",
    "    return ilist\n",
    "        \n",
    "train_treebank = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/train.French.gold.conll\")\n",
    "valid_treebank = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/dev.French.gold.conll\")\n",
    "test_treebank  = read_trees(\"/Users/bcrabbe/parsing-at-diderot/data/test.French.gold.conll\")\n",
    "\n",
    "print('#training trees',len(train_treebank))\n",
    "print('#validation trees',len(valid_treebank))\n",
    "print('#test trees',len(test_treebank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codage\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sym2idx(wordlist,unk_symbol=None):\n",
    "    \"\"\"\n",
    "    Creates a dictionary mapping symbols to integers\n",
    "    Args:\n",
    "       wordlist  (list): a list of strings\n",
    "       unk_symbol (str): a special default string for symbols unknown to this dictionary\n",
    "    Returns:\n",
    "        a dict string => idx mapping words to integer indexes\n",
    "    \"\"\"\n",
    "    if unk_symbol:\n",
    "        wordlist.append(unk_symbol)\n",
    "    wordset = set(wordlist)\n",
    "    return dict(zip(wordset,range(len(wordset)))) \n",
    "\n",
    "def code_sequence(symlist,sym2idx,unk_symbol=None):\n",
    "    \"\"\"\n",
    "    Maps a list of string to a list of int (encodes the sentence on integers) \n",
    "    Args:\n",
    "        symlist  (list): a list of strings\n",
    "        sym2idx  (dict): a dictionary mapping strings to int\n",
    "        unk_symbol(str): a special default string for the dictionary sym2idx\n",
    "    Returns a list of int as torch.tensor where words are mapped to their integer indexes.\n",
    "    \"\"\"\n",
    "    def normal_form(symbol):\n",
    "        return symbol if symbol in sym2idx else unk_symbol\n",
    "    \n",
    "    code_list = [sym2idx[normal_form(symbol)] for symbol in symlist]\n",
    "    return torch.tensor(code_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(nn.Module):\n",
    "    \n",
    "    \n",
    "    #ACTIONS\n",
    "    SHIFT = \"S\"\n",
    "    LARC  = \"L\"\n",
    "    RARC  = \"R\"\n",
    "    \n",
    "    #DUMMY TAG\n",
    "    DTAG  = 'ROOT'\n",
    "    \n",
    "    def __init__(self,embedding_dim,lstm_memory_dim,config_embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim        (int): size of word embeddings\n",
    "            lstm_memory_dim      (int): size of the sentence encoder lstm memory \n",
    "            config_embedding_dim (int): size of the hidden layer of the output MLP.\n",
    "        \"\"\"\n",
    "        super(ArcStandardParser, self).__init__()\n",
    "        self.embedding_dim        = embedding_dim\n",
    "        self.lstm_memory_dim      = lstm_memory_dim\n",
    "        self.config_embedding_dim = config_embedding_dim\n",
    "        \n",
    "    ### INDEXING ###\n",
    "    def index_symbols(self,treebank,unk_word,unk_char):\n",
    "        \"\"\"\n",
    "        Indexes the x and y symbols on integers\n",
    "        Args:\n",
    "           treebank   (list): a list of DependencyTree objects\n",
    "           unk_word (string): a symbol to use for unk words\n",
    "           unk_char (string): a symbol to use for unk char\n",
    "        Returns:\n",
    "           (...)\n",
    "        \"\"\"\n",
    "        wlist      = []   \n",
    "        taglist    = [ArcStandardParser.DTAG]\n",
    "        for dtree in treebank:\n",
    "            wlist.extend(dtree.word_list())\n",
    "            taglist.extend(dtree.tag_list())\n",
    "            \n",
    "        w2idx         = make_sym2idx(wlist,unk_symbol=unk_word)\n",
    "        tag2idx       = make_sym2idx(taglist)\n",
    "        wcounts       = Counter(wlist)\n",
    "        return w2idx,tag2idx,  _  ,wcounts  #also return actions indexes here\n",
    "    \n",
    "    ### MODEL STRUCTURE AND INFERENCES ###\n",
    "    def allocate_structure(self,nWords,nTags,nActions):\n",
    "        \"\"\"\n",
    "        Allocates memory for the parser network params.\n",
    "        Args:\n",
    "           nWords  (int):number of words in the sym2idx dict\n",
    "           nTags   (int):number of tags  in the parser\n",
    "           nActions(int):number of actions in the parser\n",
    "        \"\"\"\n",
    "        self.word_embeds           = nn.Embedding(nWords, self.embedding_dim)\n",
    "        self.tagger_bilstm         = nn.LSTM(self.embedding_dim+self.char_lstm_memory_dim, self.lstm_memory_dim,num_layers=1,bidirectional=True)\n",
    "        self.parser_bilstm         = nn.LSTM(self.lstm_memory_dim*2, self.lstm_memory_dim,num_layers=1,bidirectional=True)\n",
    "        self.tagger_lstm2hidden    = nn.Linear(self.lstm_memory_dim*2,self.config_embedding_dim)\n",
    "        self.hidden2tags           = nn.Linear(self.config_embedding_dim,nTags)\n",
    "        self.parser_lstm2hidden    = nn.Linear(self.lstm_memory_dim*8,self.config_embedding_dim)\n",
    "        self.hidden2actions        = nn.Linear(self.config_embedding_dim,nActions)\n",
    "       \n",
    "    def word_dropout(self,toklist,alpha=1.0):\n",
    "        \"\"\"\n",
    "        Replaces each word in toklist with probability Lambda / counts(w)\n",
    "        Args:\n",
    "            toklist (list): a list of strings\n",
    "            alpha  (float): a real in [0,1]. a word w is replaced with prob = alpha/counts(w)\n",
    "        Returns:\n",
    "            a list of strings with some strings replaced by the unk token\n",
    "        \"\"\"\n",
    "        return toklist #Todo\n",
    "         \n",
    "    def train_model(self,train_set,validation_set,epochs=10,learning_rate=0.1,unk_word='##UNK##',unk_char='@@',alpha=0.5):\n",
    "                    \n",
    "        self.w2idx, self.tag2idx,self.a2idx,self.wcounts = self.index_symbols(train_set,unk_word,unk_char)\n",
    "        \n",
    "        self.reva2idx   = [action for action,idx in sorted(self.a2idx.items(),key = lambda x : x[1])]\n",
    "        self.revtag2idx = [tag for tag,idx in sorted(self.tag2idx.items(),key = lambda x : x[1])]\n",
    "        \n",
    "        self.unk_word,self.unk_char = unk_word,unk_char\n",
    "        self.allocate_structure(len(self.w2idx),len(self.tag2idx),len(self.a2idx))\n",
    "        \n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer     = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        min_loss      = np.iinfo(np.int32).max\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            NLL = 0.0\n",
    "            for deptree in tqdm(train_set): \n",
    "                \n",
    "                self.zero_grad()                     \n",
    "                \n",
    "                X               = deptree.word_list()\n",
    "                xtagger,xparser = self.forward_encoding(X,alpha)\n",
    "\n",
    "                ypred,yref      = self.forward_tagger(xtagger,oracle_tree=deptree) # ypred is a sequence of y-softmaxed-probs for each step in the inference                      \n",
    "                loss            = loss_function(ypred, yref)\n",
    "                NLL            += loss.item()\n",
    "                loss.backward(retain_graph=True) #for multiobjective (allows to call loss.backward() again later on)\n",
    "                    \n",
    "                ypred,yref      = self.forward_parser(xparser,oracle_tree=deptree)\n",
    "                loss            = loss_function(ypred, yref)\n",
    "                NLL            += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            print(\"\\n[train]      Epoch %d, NLL = %f\"%(epoch,NLL/len(train_set)),file=sys.stderr)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                NLL = 0.0\n",
    "                for deptree in validation_set:\n",
    "                     X               = deptree.word_list()\n",
    "                     xtagger,xparser = self.forward_encoding(X)\n",
    "                        \n",
    "                     ypred,yref      = self.forward_tagger(xtagger,oracle_tree=deptree)      # ypred is a sequence of y-softmaxed-probs for each step in the inference\n",
    "                     loss            = loss_function(ypred, yref) \n",
    "                     NLL            += loss.item()\n",
    "                    \n",
    "                     ypred,yref      = self.forward_parser(xparser,oracle_tree=deptree)\n",
    "                     loss            = loss_function(ypred, yref)\n",
    "                     NLL            += loss.item()\n",
    "                if NLL < min_loss:\n",
    "                    torch.save(self.state_dict(), 'parsing_model.wt')        \n",
    "                print(\"[validation] Epoch %d, NLL = %f\\n\"%(epoch,NLL/len(validation_set)),file=sys.stderr)  \n",
    "            \n",
    "        self.load_state_dict(torch.load('parsing_model.wt'))\n",
    "\n",
    "        \n",
    "    def forward_encoding(self,wordlist,alpha=0.0):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for encoding the input only.\n",
    "        Args:\n",
    "           wordlist   (list): a list of strings\n",
    "           alpha     (float): 0 <= alpha <= 1 float for word dropout\n",
    "        Returns:\n",
    "           two lists of vectors encoding the sentence to be used for tagging and parsing the input\n",
    "           as torch.tensor\n",
    "        \"\"\"        \n",
    "        xwords                = code_sequence(self.word_dropout(wordlist,alpha),self.w2idx,unk_symbol=self.unk_word) \n",
    "        xword_embedded        = self.word_embeds(xwords)\n",
    "        \n",
    "        ### insert char model here ###\n",
    "        # ...\n",
    "        # ...\n",
    "        # xchar_embedded = ...\n",
    "        \n",
    "        xcodes                = xword_embedded  #torch.cat([xword_embedded,xchar_embedded],1)\n",
    "\n",
    "        xtagger, hidden_tag   = self.tagger_bilstm(xcodes.view(len(xcodes), 1, -1), None)\n",
    "        xparser, hidden_par   = self.parser_bilstm(xtagger, None)\n",
    "        return (xtagger,xparser)\n",
    "        \n",
    "        \n",
    "    def forward_tagger(self,xtagger,oracle_tree=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for tagging only.\n",
    "        Args:\n",
    "           xtagger               (list): a list of vectors,one per word,encoding the input sentence\n",
    "           oracle_tree (DependencyTree): an optional Dependency tree used as oracle\n",
    "        Returns:\n",
    "           a list of tags (one per word, including the dummy word)\n",
    "           or\n",
    "           a list of softmax distributions as torch.tensor and the list of reference tags if an oracle_tree is provided\n",
    "        \"\"\"\n",
    "        \n",
    "        hidden        = F.relu(self.tagger_lstm2hidden(xtagger.squeeze()))\n",
    "        softmaxin     = self.hidden2tags(hidden)\n",
    "        log_softmaxes = F.log_softmax(softmaxin, dim=1)  \n",
    "\n",
    "        if oracle_tree:\n",
    "            ref_tags = [ArcStandardParser.DTAG] + oracle_tree.tag_list()\n",
    "            ref_tags = code_sequence(ref_tags,self.tag2idx)\n",
    "            return (log_softmaxes,ref_tags)\n",
    "        else:\n",
    "            max_probs,  argmaxlist  = torch.max(log_softmaxes,1)\n",
    "            pred_tags               = [self.revtag2idx[argmax] for argmax in argmaxlist]\n",
    "            return pred_tags\n",
    "        \n",
    "    def forward_parser(self,xparser,oracle_tree=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for parsing only.\n",
    "        Args:\n",
    "           xparser               (list): a list of vectors,one per word,encoding the input sentence\n",
    "           oracle_tree (DependencyTree): an optional Dependency tree used to drive the oracle\n",
    "        Returns:\n",
    "           the final configuration\n",
    "           or\n",
    "           a list of softmax distributions, and the list of reference actions if an oracle_tree is provided\n",
    "        \"\"\"        \n",
    "        #TODO\n",
    "        pass\n",
    "       \n",
    "        \n",
    "    def predict_tree(self,wordlist):\n",
    "        \"\"\"\n",
    "        Actually performs the tagging and parsing of a sentence.\n",
    "        Args:\n",
    "            wordlist (list): a list of strings\n",
    "        Returns:\n",
    "            a tree  (DependencyTree): the dep tree and its tags\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            xtagger,xparser  = self.forward_encoding(wordlist)\n",
    "            taglist          = self.forward_tagger(xtagger)\n",
    "            pass #TODO\n",
    "        \n",
    "    ### TRANSITION SYSTEM ###\n",
    "    def init_configuration(self,N):\n",
    "        \"\"\"\n",
    "        Creates an init configuration for an arc standard parser.\n",
    "        A configuration is a quadruple (stack_idxes,buffer_idxes,dep_arcs)\n",
    "        Args:\n",
    "            N  (int): the length of the input\n",
    "        Returns:\n",
    "            the init configuration\n",
    "        \"\"\"\n",
    "        return ([0],list(range(1,N)),[]) \n",
    "        # configs stack and buffer are filled with integer indexes of word positions\n",
    "        # stack starts with [0] the dummy root node\n",
    "        \n",
    "    def shift(self,configuration):\n",
    "        \"\"\"\n",
    "        Performs shift on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration to be shifted\n",
    "        Returns: \n",
    "           the shifted configuration\n",
    "        \"\"\"\n",
    "        pass #TODO\n",
    "                \n",
    "    def left_arc(self,configuration,deplabel):\n",
    "        \"\"\"\n",
    "        Performs a left arc action on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "           deplabel     (string): a string labelling the arc\n",
    "        Returns: \n",
    "           the resulting configuration\n",
    "        \"\"\"\n",
    "        pass #TODO\n",
    "    \n",
    "    def right_arc(self,configuration,deplabel):\n",
    "        \"\"\"\n",
    "        Performs a right arc action on a configuration and returns the result\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "           deplabel     (string): a string labelling the arc\n",
    "        Returns: \n",
    "           the resulting configuration\n",
    "        \"\"\"\n",
    "        pass #TODO\n",
    "    \n",
    "    def run_time_mask(self,configuration,log_mask=True):\n",
    "        \"\"\"\n",
    "        The run time mask returns a mask where impossible actions have 0 score.\n",
    "        The mask is a vector of mask pseudo scores with dimensions comparable to the output action score vector.\n",
    "        Impossible actions are filled with a -inf value, and possible actions with a 0 value.\n",
    "        \n",
    "        The mask can be added to action scores prior to softmax.\n",
    "        Using the mask ensures that we have no fatal error while performing oracle-less runtime search.\n",
    "        Args:\n",
    "           configuration (tuple): a configuration\n",
    "        Kwargs:\n",
    "            log_mask      (bool): bool stating if the mask is in logit_space or in probabilistic space.\n",
    "        Returns:\n",
    "            torch.tensor of size num Actions filled with masking values\n",
    "        \"\"\"\n",
    "        m0,m1 = (-float('inf'), 0.0) if log_mask else (0.0,1.0)\n",
    "        \n",
    "        def valmask(atype,stack_size,buffer_size,configuration):\n",
    "            if atype in [ArcStandardParser.LARC,ArcStandardParser.RARC] and stack_size < 2:\n",
    "                return m0\n",
    "            elif atype == ArcStandardParser.LARC and stack_size >= 2:\n",
    "                S,B,A = configuration\n",
    "                if S[-2] == 0:\n",
    "                    return m0\n",
    "            elif atype == ArcStandardParser.SHIFT and buffer_size == 0:\n",
    "                return m0\n",
    "            return m1\n",
    "            \n",
    "        S,B,A       = configuration\n",
    "        stack_size  = len(S)\n",
    "        buffer_size = len(B)\n",
    "        return torch.tensor([ valmask(atype,stack_size,buffer_size,configuration)   for (atype,alabel) in self.reva2idx])  \n",
    "\n",
    "    def oracle(self,config,deptree):\n",
    "        \"\"\"\n",
    "        Deterministic function translating a dependency tree into a derivation sequence step by step.\n",
    "        Arc standard static oracle.\n",
    "        Args:\n",
    "            config           (tuple): a configuration\n",
    "            deptree (DependencyTree): the tree used as ground truth\n",
    "        Returns:\n",
    "            the action as a tuple (ActionType,ActionLabel)\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        pass \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_parser(model,test_treebank):\n",
    "    \n",
    "    las_correct = 0\n",
    "    uas_correct = 0\n",
    "    tag_correct = 0\n",
    "    \n",
    "    for dtree in test_treebank:\n",
    "        \n",
    "        pass #TODO\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = ArcStandardParser(32,64,100)\n",
    "m.train_model(train_treebank,valid_treebank,epochs=3,learning_rate=0.001,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_parser(m,test_treebank)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
