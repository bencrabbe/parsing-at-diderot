{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enoncé du TD (Analyse de sentiment)\n",
    "===================\n",
    "Le TD consiste à finir l'implémentation d'un prédicteur naïf de la polarité d'une critique de film tirée de IMDB \n",
    "anglais.\n",
    "\n",
    "Le jeu de données est à télécharger depuis le site suivant : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Le TD consiste à compléter les trous laissés dans ce notebook :\n",
    "\n",
    "* Remplacer les chemins d'accès aux données codés en dur dans le notebook\n",
    "* Implémenter le chargement de données de test dans le notebook dans la cellule marquée à cet effet\n",
    "* Compléter la méthode `run_test(...)` de la classe `SentimentAnalyser` pour qu'elle renvoie un score d'accurracy de classification sur un jeu de test.\n",
    "* Découper les données de test en données de validation (15000 premiers exemples) et de test final (10000 derniers exemples)\n",
    "* Augmenter la méthode `train(...)` de la classe `SentimentAnalyser` pour qu'elle prenne en second argument les données de validation. Le corps de la méthode sera modifié pour (1) réaliser une évaluation sur les données de validation à chaque époque (2) sauvegarder au final le modèle qui minimise la perte sur les données de validation. \n",
    "* Faire une recherche d'hyper-paramètres (nombre d'épochs et learning rate).\n",
    "\n",
    "Le rendu attendu est une copie de ce notebook qui aura été complétée. Profitez du framework notebook pour commenter vos réponses si besoin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "#loads data from disk\n",
    "def load_dataset(dir_path,ref_label):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a directory path and \n",
    "    returns a list of couples (Counter of Bow_freq,ref_label) one for each text\n",
    "    \"\"\"\n",
    "    dpath    = os.path.abspath(dir_path)\n",
    "    data_set = [] \n",
    "    for f in os.listdir(dpath):\n",
    "        filepath    = os.path.join(dpath,f)\n",
    "        file_stream = open(filepath)\n",
    "        text        = file_stream.read().split()\n",
    "        file_stream.close()\n",
    "        data_set.append((Counter(text),ref_label))\n",
    "    return data_set\n",
    "    \n",
    "trainpos = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/train/pos\",1)\n",
    "trainneg = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/train/neg\",0)\n",
    "dataset  = trainpos\n",
    "dataset.extend(trainneg)\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A FAIRE : implementer ici le chargement des donnees de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a dict that maps each known words string to a unique integer\n",
    "def make_w2idx(dataset):\n",
    "    wordset = set([])\n",
    "    for X,Y in dataset:\n",
    "        wordset.update([word for word in X])\n",
    "    return dict(zip(wordset,range(len(wordset))))   \n",
    "\n",
    "def vectorize_text(counter,w2idx):\n",
    "    xvec = torch.zeros(len(w2idx))\n",
    "    for word in counter:\n",
    "        if word in w2idx:       #manages unk words (ignored)\n",
    "            xvec[w2idx[word]] = counter[word] \n",
    "    return xvec.squeeze()\n",
    "\n",
    "def vectorize_target(ylabel):\n",
    "     return torch.tensor(float(ylabel))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SentimentAnalyzer(nn.Module): \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(SentimentAnalyzer, self).__init__()\n",
    "            self.reset_structure(1,1)\n",
    "            \n",
    "        def reset_structure(self,vocab_size, num_labels):\n",
    "            super(SentimentAnalyzer, self).__init__()\n",
    "            self.W = nn.Linear(vocab_size, num_labels)\n",
    "            \n",
    "        def forward(self, text_vec):\n",
    "            return F.sigmoid(self.W(text_vec)) #sigmoid is the logistic activation\n",
    "        \n",
    "        def train(self,train_set,learning_rate,epochs):\n",
    "            self.w2idx = make_w2idx(dataset)\n",
    "            self.reset_structure(len(self.w2idx),1)\n",
    "            loss_func  = nn.BCELoss() #remind that minimizing Binary Cross Entropy <=> minimizing NLL\n",
    "            optimizer  = optim.SGD(self.parameters(), lr=0.1)\n",
    "            for epoch in range(epochs):\n",
    "                global_logloss = 0.0\n",
    "                for X, Y in train_set: \n",
    "                    self.zero_grad()\n",
    "                    xvec            = vectorize_text(X,self.w2idx)\n",
    "                    yvec            = vectorize_target(Y)\n",
    "                    prob            = self(xvec).squeeze()\n",
    "                    loss            = loss_func(prob,yvec)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    global_logloss += loss.item()\n",
    "                print(\"Epoch %d, mean cross entropy = %f\"%(epoch,global_logloss/len(train_set)))\n",
    "\n",
    "        def run_test(self,test_set):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                pass  #implementer ici le corps de la fonction\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean cross entropy = 7.189321\n",
      "Epoch 1, mean cross entropy = 5.236371\n",
      "Epoch 2, mean cross entropy = 4.378302\n",
      "Epoch 3, mean cross entropy = 4.029399\n",
      "Epoch 4, mean cross entropy = 3.757117\n",
      "Epoch 5, mean cross entropy = 3.545291\n",
      "Epoch 6, mean cross entropy = 3.418714\n",
      "Epoch 7, mean cross entropy = 3.049814\n",
      "Epoch 8, mean cross entropy = 2.657483\n",
      "Epoch 9, mean cross entropy = 2.710106\n"
     ]
    }
   ],
   "source": [
    "sent = SentimentAnalyzer()\n",
    "sent.train(dataset,0.1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executer le test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
