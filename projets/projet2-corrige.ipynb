{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enoncé du TD (Analyse de sentiment, le retour)\n",
    "=========================\n",
    "Le TD consiste à redéfinir le projet 1 : prédicteur naïf de la polarité d'une critique de film tirée de IMDB \n",
    "anglais. Le modèle est un modèle de régression logistique traditionnel implémenté avec pytorch mais cette fois-ci le texte est représenté à l'aide d'un LSTM.\n",
    "\n",
    "Le jeu de données est à télécharger depuis le site suivant : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Le TD consiste à remplacer le modèle utilisé dans le TD 1 (où $\\Phi(\\mathbf{x})$ représente un vecteur de fréquences des mots dans un texte):\n",
    "\\begin{align}\n",
    "P(Y=1 | \\Phi(\\mathbf{x}) ;  \\boldsymbol\\theta) &= \\frac{exp(\\mathbf{w}^T\\Phi(\\mathbf{x}))}{1+exp(\\mathbf{w}^T\\Phi(\\mathbf{x}))}\n",
    "\\end{align}\n",
    "par le modèle suivant :\n",
    "\\begin{align}\n",
    "P(Y=1 | \\mathbf{x} ;  \\boldsymbol\\theta) &= \\frac{exp(\\mathbf{w}^T \\mathbf{h})}{1+exp(\\mathbf{w}^T\\mathbf{h})}\\\\\n",
    "\\mathbf{h} &= \\text{LSTM}(\\mathbf{e})\\\\\n",
    "\\mathbf{e} &= \\text{embedding}(\\mathbf{x})\n",
    "\\end{align}\n",
    "\n",
    "où $\\mathbf{e}$ est un vecteur d'embeddings (un embedding par mot du texte à traiter) mais $\\mathbf{h}$ est un vecteur de réels qui représente la mémoire du LSTM lors de la dernière prédiction.\n",
    "\n",
    "Plan détaillé :\n",
    " * Observez que la fonction de lecture de données ne renvoie plus un dictionnaire mais une liste des mots présents dans le texte\n",
    " * Remplissez la fonction `code_sequence` pour qu'elle renvoie une liste d'entiers qui correspond à la séquence des codes de mots du texte. La fonction renvoie cette liste sous forme de `torch.tensor`.\n",
    " * Mettez à jour les méthodes du modèle pour coder le texte à l'aide d'un LSTM. Il s'agit des méthodes : `allocate_structure`, `forward` et éventuellement `train`. \n",
    " * Vous pouvez également remplacer le LSTM par un Bi-LSTM, mais c'est un peu plus difficile à programmer (voir sur internet).\n",
    " * Vous verrez que l'entrainement est très long si réalisé naivement. Vous pouvez mettre au point une méthode qui (a) filtre le vocabulaire pour réduire la taille des textes et qui (b) organise l'entrainement en \"mini-batchs\" (voir sur internet, **exercice plus difficile**)\n",
    " \n",
    "Le rendu attendu est une copie de ce notebook qui aura été complétée. Profitez du framework notebook pour commenter vos réponses si besoin.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "#loads data from disk\n",
    "def load_dataset(dir_path,ref_label):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a directory path and \n",
    "    returns a list of couples (list of words,ref_label) one for each text\n",
    "    \"\"\"\n",
    "    dpath    = os.path.abspath(dir_path)\n",
    "    data_set = [] \n",
    "    for f in os.listdir(dpath):\n",
    "        filepath    = os.path.join(dpath,f)\n",
    "        file_stream = open(filepath)\n",
    "        text        = file_stream.read().split()\n",
    "        data_set.append((text,ref_label))\n",
    "        file_stream.close()\n",
    "    return data_set\n",
    "    \n",
    "trainpos = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/train/pos\",1)\n",
    "trainneg = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/train/neg\",0)\n",
    "trainset = trainpos\n",
    "trainset.extend(trainneg)\n",
    "random.shuffle(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpos = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/test/pos\",1)\n",
    "testneg = load_dataset(\"/Users/bcrabbe/parsing-at-diderot/data/aclImdb/test/neg\",0)\n",
    "testset = testpos\n",
    "testset.extend(testneg)\n",
    "random.shuffle(testset)\n",
    "\n",
    "trainset = trainset\n",
    "validset = testset[:5000]\n",
    "testset  = testset[5000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#creates a dict that maps each known word string to a unique integer\n",
    "def make_w2idx(dataset,vocab_size=None,unk_symbol=None):\n",
    "    wordcounts = Counter()\n",
    "    for X,Y in dataset:\n",
    "        wordcounts.update([word for word in X])\n",
    "        \n",
    "    print('Capping a vocab of size %d to size %d'%(len(wordcounts),vocab_size))\n",
    "    wordset =[ word for word,c in wordcounts.most_common(vocab_size) ]\n",
    "    \n",
    "    if unk_symbol:\n",
    "        wordset.append(unk_symbol)\n",
    "        \n",
    "    return dict(zip(wordset,range(len(wordset))))   \n",
    "\n",
    "def code_sequence(symlist,w2idx,unk_symbol=None,maxlen=100):       \n",
    "    def normal_form(symbol):\n",
    "        return symbol if symbol in w2idx else unk_symbol\n",
    "    \n",
    "    if maxlen > 0:\n",
    "        symlist = symlist[:maxlen]\n",
    "    \n",
    "    code_list = [ w2idx[normal_form(symbol)] for symbol in symlist ]\n",
    "    return torch.tensor(code_list, dtype=torch.long)\n",
    "\n",
    "def vectorize_target(ylabel):\n",
    "     return torch.tensor(float(ylabel))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SentimentAnalyzer(nn.Module): \n",
    "        def __init__(self, embedding_dim,lstm_memory_dim):\n",
    "            \n",
    "            super(SentimentAnalyzer, self).__init__()\n",
    "            self.embedding_dim        = embedding_dim\n",
    "            self.lstm_memory_dim      = lstm_memory_dim\n",
    "            self.allocate_structure(1,1)\n",
    "           \n",
    "        \n",
    "        def allocate_structure(self,vocab_size, num_labels):\n",
    "            \n",
    "            self.E      = nn.Embedding(vocab_size,self.embedding_dim)\n",
    "            self.bilstm = nn.LSTM(self.embedding_dim, self.lstm_memory_dim,num_layers=1,bidirectional=True) \n",
    "            self.W      = nn.Linear(self.lstm_memory_dim*2, num_labels)    \n",
    "             \n",
    "            \n",
    "        def forward(self, xinput):\n",
    "            xembedded                    = self.E(xinput)\n",
    "            lstm_out, (hiddwen,cell)      = self.bilstm(xembedded.view(len(xinput), 1, -1), None)\n",
    "            return F.sigmoid(self.W(hidden.view(1,self.lstm_memory_dim*2))) #sigmoid is the logistic activation\n",
    "        \n",
    "        def train(self,train_set,dev_set,learning_rate,epochs,vocab_size=1000,unk_symbol='<unk>'):\n",
    "            \n",
    "            self.w2idx = make_w2idx(train_set,vocab_size,unk_symbol)\n",
    "            self.unk   = unk_symbol\n",
    "            self.allocate_structure(len(self.w2idx),1)\n",
    "            loss_func  = nn.BCELoss() #remind that minimizing Binary Cross Entropy <=> minimizing NLL\n",
    "            optimizer  = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "            \n",
    "            min_loss = np.iinfo(np.int32).max\n",
    "            for epoch in range(epochs):\n",
    "                global_logloss = 0.0\n",
    "                for X, Y in train_set: \n",
    "                    self.zero_grad()\n",
    "                    xvec            = code_sequence(X,self.w2idx,unk_symbol)\n",
    "                    yvec            = vectorize_target(Y)\n",
    "                    prob            = self.forward(xvec).squeeze()\n",
    "                    loss            = loss_func(prob,yvec)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    global_logloss += loss.item()\n",
    "                print(\"Epoch %d, mean cross entropy (train) = %f\"%(epoch,global_logloss/len(train_set)))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    global_logloss = 0.0\n",
    "                    for X, Y in dev_set: \n",
    "                        xvec            = code_sequence(X,self.w2idx,unk_symbol)\n",
    "                        yvec            = vectorize_target(Y)\n",
    "                        prob            = self(xvec).squeeze()\n",
    "                        loss            = loss_func(prob,yvec)\n",
    "                        global_logloss += loss.item()\n",
    "                    print(\"Epoch %d, mean cross entropy (dev)   = %f\\n\"%(epoch,global_logloss/len(dev_set)))\n",
    "                    if global_logloss < min_loss:\n",
    "                        torch.save(self.state_dict(), 'sentiment_model.wt')\n",
    "            self.load_state_dict(torch.load('sentiment_model.wt'))\n",
    "            \n",
    "        def run_test(self,test_set):\n",
    "            \n",
    "            ncorrect = 0\n",
    "            with torch.no_grad():\n",
    "                for X, Y in test_set: \n",
    "                    xvec            = code_sequence(X,self.w2idx,self.unk)\n",
    "                    prob            = self(xvec).squeeze()\n",
    "                    if Y == 1 and prob > 0.5:\n",
    "                        ncorrect += 1\n",
    "                    if Y == 0 and prob <= 0.5: \n",
    "                        ncorrect += 1\n",
    "            print(\"Test Accurracy\",ncorrect/len(test_set))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capping a vocab of size 280617 to size 5000\n",
      "Epoch 0, mean cross entropy (train) = 0.610629\n",
      "Epoch 0, mean cross entropy (dev)   = 0.483787\n",
      "\n",
      "Epoch 1, mean cross entropy (train) = 0.427120\n",
      "Epoch 1, mean cross entropy (dev)   = 0.434813\n",
      "\n",
      "Epoch 2, mean cross entropy (train) = 0.342622\n",
      "Epoch 2, mean cross entropy (dev)   = 0.470207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent = SentimentAnalyzer(32,64)\n",
    "sent.train(trainset,validset,0.001,3,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accurracy 0.7865\n"
     ]
    }
   ],
   "source": [
    "sent.run_test(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
